//! ralph init — Create workspace: ralph.yml, .ralph/, PROMPT.md

const std = @import("std");
const context_mod = @import("../../framework/context.zig");
const utils = @import("../../utils/mod.zig");
const cli_io = utils.io_backend;
const cfg = @import("config.zig");

const RALPH_YML_TEMPLATE =
    \\# Ralph Orchestrator Configuration
    \\# Generated by: abi ralph init
    \\
    \\cli:
    \\  backend: "{s}"
    \\
    \\llm:
    \\  backend: "{s}"
    \\  fallback: "mlx,ollama,lm_studio,vllm,plugin_http,plugin_native"
    \\  strict_backend: false
    \\  model: "llama3.2"
    \\  plugin: ""
    \\
    \\event_loop:
    \\  prompt_file: "PROMPT.md"
    \\  completion_promise: "LOOP_COMPLETE"
    \\  max_iterations: 5
    \\
    \\execution:
    \\  max_iterations: 5
    \\  max_fix_attempts: 2
    \\  require_clean_tree: true
    \\
    \\gates:
    \\  per_iteration: "zig build verify-all"
    \\
;

const PROMPT_MD_TEMPLATE =
    \\# Ralph Task
    \\
    \\<!-- Replace this with your task description. -->
    \\
    \\## Goal
    \\
    \\Describe what you want Ralph to accomplish here.
    \\
    \\## Acceptance Criteria
    \\
    \\- [ ] Criterion 1
    \\- [ ] Criterion 2
    \\
;

pub fn runInit(ctx: *const context_mod.CommandContext, args: []const [:0]const u8) !void {
    const allocator = ctx.allocator;
    var backend: []const u8 = "llama_cpp";
    var force = false;

    var i: usize = 0;
    while (i < args.len) : (i += 1) {
        const arg = std.mem.sliceTo(args[i], 0);
        if (std.mem.eql(u8, arg, "--backend") or std.mem.eql(u8, arg, "-b")) {
            i += 1;
            if (i < args.len) backend = std.mem.sliceTo(args[i], 0);
        } else if (std.mem.eql(u8, arg, "--force") or std.mem.eql(u8, arg, "-f")) {
            force = true;
        } else if (utils.args.matchesAny(arg, &[_][]const u8{ "--help", "-h", "help" })) {
            std.debug.print(
                \\Usage: abi ralph init [options]
                \\
                \\Create a Ralph workspace in the current directory.
                \\
                \\Options:
                \\  -b, --backend <name>  LLM backend (default: llama_cpp)
                \\  -f, --force           Overwrite existing workspace
                \\  -h, --help            Show this help
                \\
            , .{});
            return;
        }
    }

    var io_backend = cli_io.initIoBackend(allocator);
    defer io_backend.deinit();
    const io = io_backend.io();

    if (cfg.fileExists(io, cfg.WORKSPACE_DIR ++ "/state.json") and !force) {
        std.debug.print(
            "Workspace already exists (.ralph/). Use --force to reinitialize.\n",
            .{},
        );
        return;
    }

    // Create directory tree
    cfg.ensureDir(io, cfg.WORKSPACE_DIR);
    cfg.ensureDir(io, cfg.AGENT_DIR);
    cfg.ensureDir(io, cfg.LOGS_DIR);
    cfg.ensureDir(io, cfg.RUNS_DIR);

    // Write ralph.yml
    const yml = try std.fmt.allocPrint(allocator, RALPH_YML_TEMPLATE, .{ backend, backend });
    defer allocator.free(yml);
    try cfg.writeFile(allocator, io, cfg.CONFIG_FILE, yml);

    // Write PROMPT.md (only if missing or force)
    if (!cfg.fileExists(io, cfg.PROMPT_FILE) or force) {
        try cfg.writeFile(allocator, io, cfg.PROMPT_FILE, PROMPT_MD_TEMPLATE);
    }

    // Write initial state
    cfg.writeState(allocator, io, .{});

    std.debug.print(
        \\Ralph workspace initialized.
        \\
        \\  ralph.yml   — configuration (backend: {s})
        \\  PROMPT.md   — edit this with your task
        \\  .ralph/     — runtime state directory
        \\
        \\Next steps:
        \\  1. Edit PROMPT.md with your task description
        \\  2. Run: abi ralph run
        \\
    , .{backend});
}
