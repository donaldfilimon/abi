//! Continuous Performance Monitoring for WDBX
//!
//! Monitors system performance in real-time and alerts on degradation

const std = @import("std");
const abi = @import("abi");

const Config = struct {
    interval_seconds: u64 = 60,
    threshold_percent: f64 = 10.0,
    baseline_file: []const u8 = "performance_baseline.json",
    alert_webhook: ?[]const u8 = null,
    metrics_endpoint: []const u8 = "http://localhost:8080/stats",
    enable_logging: bool = true,
    log_file: []const u8 = "performance_monitor.log",
};

const PerformanceMetrics = struct {
    timestamp: i64,
    operations_per_sec: f64,
    average_latency_us: f64,
    p99_latency_us: f64,
    memory_usage_mb: f64,
    cpu_usage_percent: f64,
    active_connections: u32,
    error_rate: f64,
};

const Baseline = struct {
    ops_per_sec: f64,
    avg_latency_us: f64,
    p99_latency_us: f64,
    memory_usage_mb: f64,
    established_at: i64,
};

pub fn main() !void {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    // Parse command line arguments
    const args = try std.process.argsAlloc(allocator);
    defer std.process.argsFree(allocator, args);

    var config = Config{};
    var i: usize = 1;
    while (i < args.len) : (i += 1) {
        if (std.mem.eql(u8, args[i], "--interval") and i + 1 < args.len) {
            i += 1;
            config.interval_seconds = try std.fmt.parseInt(u64, args[i], 10);
        } else if (std.mem.eql(u8, args[i], "--threshold") and i + 1 < args.len) {
            i += 1;
            config.threshold_percent = try std.fmt.parseFloat(f64, args[i]);
        } else if (std.mem.eql(u8, args[i], "--baseline") and i + 1 < args.len) {
            i += 1;
            config.baseline_file = args[i];
        } else if (std.mem.eql(u8, args[i], "--webhook") and i + 1 < args.len) {
            i += 1;
            config.alert_webhook = args[i];
        } else if (std.mem.eql(u8, args[i], "--endpoint") and i + 1 < args.len) {
            i += 1;
            config.metrics_endpoint = args[i];
        } else if (std.mem.eql(u8, args[i], "--no-log")) {
            config.enable_logging = false;
        } else if (std.mem.eql(u8, args[i], "--help")) {
            printHelp();
            return;
        }
    }

    std.debug.print("ðŸ” WDBX Continuous Performance Monitor\n", .{});
    std.debug.print("=====================================\n", .{});
    std.debug.print("Monitoring endpoint: {s}\n", .{config.metrics_endpoint});
    std.debug.print("Check interval: {} seconds\n", .{config.interval_seconds});
    std.debug.print("Alert threshold: {d:.1}%\n", .{config.threshold_percent});
    if (config.alert_webhook) |webhook| {
        std.debug.print("Alert webhook: {s}\n", .{webhook});
    }
    std.debug.print("\n", .{});

    // Load or establish baseline
    const baseline = try loadOrEstablishBaseline(allocator, config);
    std.debug.print("ðŸ“Š Performance Baseline:\n", .{});
    std.debug.print("  Operations/sec: {d:.0}\n", .{baseline.ops_per_sec});
    std.debug.print("  Avg latency: {d:.0}Î¼s\n", .{baseline.avg_latency_us});
    std.debug.print("  P99 latency: {d:.0}Î¼s\n", .{baseline.p99_latency_us});
    std.debug.print("  Memory usage: {d:.1}MB\n", .{baseline.memory_usage_mb});
    std.debug.print("\n", .{});

    // Open log file if enabled
    var log_file: ?std.fs.File = null;
    if (config.enable_logging) {
        log_file = try std.fs.cwd().createFile(config.log_file, .{ .truncate = false });
        try log_file.?.seekFromEnd(0);
    }
    defer if (log_file) |f| f.close();

    // Main monitoring loop
    std.debug.print("ðŸš€ Starting continuous monitoring... (Press Ctrl+C to stop)\n\n", .{});

    var consecutive_alerts: u32 = 0;
    while (true) {
        // Collect current metrics
        const metrics = try collectMetrics(allocator, config.metrics_endpoint);

        // Check for degradation
        const degradation = checkDegradation(metrics, baseline, config.threshold_percent);

        // Display current status
        displayStatus(metrics, baseline, degradation);

        // Log metrics if enabled
        if (log_file) |file| {
            try logMetrics(file, metrics, degradation);
        }

        // Handle alerts
        if (degradation.any()) {
            consecutive_alerts += 1;
            if (consecutive_alerts >= 3) { // Alert after 3 consecutive degradations
                try sendAlert(allocator, config, metrics, baseline, degradation);
            }
        } else {
            consecutive_alerts = 0;
        }

        // Sleep until next check
        std.Thread.sleep(config.interval_seconds * std.time.ns_per_s);
    }
}

const Degradation = struct {
    ops_degraded: bool = false,
    latency_degraded: bool = false,
    p99_degraded: bool = false,
    memory_degraded: bool = false,

    fn any(self: Degradation) bool {
        return self.ops_degraded or self.latency_degraded or
            self.p99_degraded or self.memory_degraded;
    }
};

fn checkDegradation(current: PerformanceMetrics, baseline: Baseline, threshold: f64) Degradation {
    var result = Degradation{};

    // Check operations/sec (lower is bad)
    const ops_diff = (baseline.ops_per_sec - current.operations_per_sec) / baseline.ops_per_sec * 100;
    if (ops_diff > threshold) {
        result.ops_degraded = true;
    }

    // Check average latency (higher is bad)
    const latency_diff = (current.average_latency_us - baseline.avg_latency_us) / baseline.avg_latency_us * 100;
    if (latency_diff > threshold) {
        result.latency_degraded = true;
    }

    // Check P99 latency
    const p99_diff = (current.p99_latency_us - baseline.p99_latency_us) / baseline.p99_latency_us * 100;
    if (p99_diff > threshold) {
        result.p99_degraded = true;
    }

    // Check memory usage
    const memory_diff = (current.memory_usage_mb - baseline.memory_usage_mb) / baseline.memory_usage_mb * 100;
    if (memory_diff > threshold * 2) { // Higher threshold for memory
        result.memory_degraded = true;
    }

    return result;
}

fn displayStatus(metrics: PerformanceMetrics, baseline: Baseline, degradation: Degradation) void {
    const timestamp = std.time.timestamp();
    std.debug.print("[{d}] Performance Status:\n", .{timestamp});

    // Operations/sec
    const ops_symbol = if (degradation.ops_degraded) "âŒ" else "âœ…";
    const ops_change = (metrics.operations_per_sec - baseline.ops_per_sec) / baseline.ops_per_sec * 100;
    std.debug.print("  {s} Ops/sec: {d:.0} ({s}{d:.1}%)\n", .{ ops_symbol, metrics.operations_per_sec, if (ops_change >= 0) "+" else "", ops_change });

    // Latency
    const latency_symbol = if (degradation.latency_degraded) "âŒ" else "âœ…";
    const latency_change = (metrics.average_latency_us - baseline.avg_latency_us) / baseline.avg_latency_us * 100;
    std.debug.print("  {s} Avg Latency: {d:.0}Î¼s ({s}{d:.1}%)\n", .{ latency_symbol, metrics.average_latency_us, if (latency_change >= 0) "+" else "", latency_change });

    // P99 Latency
    const p99_symbol = if (degradation.p99_degraded) "âŒ" else "âœ…";
    const p99_change = (metrics.p99_latency_us - baseline.p99_latency_us) / baseline.p99_latency_us * 100;
    std.debug.print("  {s} P99 Latency: {d:.0}Î¼s ({s}{d:.1}%)\n", .{ p99_symbol, metrics.p99_latency_us, if (p99_change >= 0) "+" else "", p99_change });

    // Memory
    const memory_symbol = if (degradation.memory_degraded) "âŒ" else "âœ…";
    const memory_change = (metrics.memory_usage_mb - baseline.memory_usage_mb) / baseline.memory_usage_mb * 100;
    std.debug.print("  {s} Memory: {d:.1}MB ({s}{d:.1}%)\n", .{ memory_symbol, metrics.memory_usage_mb, if (memory_change >= 0) "+" else "", memory_change });

    // Additional info
    std.debug.print("  ðŸ“Š CPU: {d:.1}% | Connections: {} | Errors: {d:.2}%\n", .{
        metrics.cpu_usage_percent,
        metrics.active_connections,
        metrics.error_rate * 100,
    });

    std.debug.print("\n", .{});
}

fn collectMetrics(allocator: std.mem.Allocator, endpoint: []const u8) !PerformanceMetrics {
    // In a real implementation, this would fetch from the HTTP endpoint
    // For now, we'll simulate with realistic values
    _ = allocator;
    _ = endpoint;

    const random = std.crypto.random;

    return PerformanceMetrics{
        .timestamp = std.time.milliTimestamp(),
        .operations_per_sec = 2500.0 + @as(f64, @floatFromInt(random.intRangeAtMost(i32, -200, 200))),
        .average_latency_us = 800.0 + @as(f64, @floatFromInt(random.intRangeAtMost(i32, -100, 200))),
        .p99_latency_us = 2000.0 + @as(f64, @floatFromInt(random.intRangeAtMost(i32, -200, 500))),
        .memory_usage_mb = 150.0 + @as(f64, @floatFromInt(random.intRangeAtMost(i32, -10, 30))),
        .cpu_usage_percent = 25.0 + @as(f64, @floatFromInt(random.intRangeAtMost(i32, -10, 10))),
        .active_connections = random.intRangeAtMost(u32, 50, 150),
        .error_rate = @as(f64, @floatFromInt(random.intRangeAtMost(u32, 0, 5))) / 1000.0,
    };
}

fn loadOrEstablishBaseline(allocator: std.mem.Allocator, config: Config) !Baseline {
    // Try to load existing baseline
    const file = std.fs.cwd().openFile(config.baseline_file, .{}) catch |err| {
        if (err == error.FileNotFound) {
            // Establish new baseline
            std.debug.print("ðŸ“ˆ Establishing new performance baseline...\n", .{});
            return establishNewBaseline(allocator, config.metrics_endpoint);
        }
        return err;
    };
    defer file.close();

    // Manually read entire file with a growable buffer
    var capacity: usize = 4096;
    var buffer = try allocator.alloc(u8, capacity);
    errdefer allocator.free(buffer);

    var total_read: usize = 0;
    while (true) {
        var chunk: [2048]u8 = undefined;
        const n = try file.read(&chunk);
        if (n == 0) break;
        if (total_read + n > capacity) {
            const new_capacity = if (capacity * 2 > total_read + n) capacity * 2 else total_read + n;
            buffer = try allocator.realloc(buffer, new_capacity);
            capacity = new_capacity;
        }
        @memcpy(buffer[total_read .. total_read + n], chunk[0..n]);
        total_read += n;
    }

    const content = buffer[0..total_read];
    defer allocator.free(buffer);

    // Parse JSON baseline (simplified for example)
    _ = content; // placeholder until JSON parsing is implemented
    return Baseline{
        .ops_per_sec = 2777.0,
        .avg_latency_us = 800.0,
        .p99_latency_us = 2000.0,
        .memory_usage_mb = 150.0,
        .established_at = std.time.milliTimestamp(),
    };
}

fn establishNewBaseline(allocator: std.mem.Allocator, endpoint: []const u8) !Baseline {
    // Collect multiple samples and average them
    var total_ops: f64 = 0;
    var total_latency: f64 = 0;
    var total_p99: f64 = 0;
    var total_memory: f64 = 0;

    const samples = 10;
    for (0..samples) |_| {
        const metrics = try collectMetrics(allocator, endpoint);
        total_ops += metrics.operations_per_sec;
        total_latency += metrics.average_latency_us;
        total_p99 += metrics.p99_latency_us;
        total_memory += metrics.memory_usage_mb;

        std.Thread.sleep(std.time.ns_per_s);
    }

    return Baseline{
        .ops_per_sec = total_ops / @as(f64, @floatFromInt(samples)),
        .avg_latency_us = total_latency / @as(f64, @floatFromInt(samples)),
        .p99_latency_us = total_p99 / @as(f64, @floatFromInt(samples)),
        .memory_usage_mb = total_memory / @as(f64, @floatFromInt(samples)),
        .established_at = std.time.milliTimestamp(),
    };
}

fn logMetrics(file: std.fs.File, metrics: PerformanceMetrics, degradation: Degradation) !void {
    var buf: [1024]u8 = undefined;
    const formatted = try std.fmt.bufPrint(buf[0..], "{d},{d:.0},{d:.0},{d:.0},{d:.1},{d:.1},{},{d:.4},{}\n", .{
        metrics.timestamp,
        metrics.operations_per_sec,
        metrics.average_latency_us,
        metrics.p99_latency_us,
        metrics.memory_usage_mb,
        metrics.cpu_usage_percent,
        metrics.active_connections,
        metrics.error_rate,
        degradation.any(),
    });
    _ = try file.writeAll(formatted);
}

fn sendAlert(allocator: std.mem.Allocator, config: Config, metrics: PerformanceMetrics, baseline: Baseline, degradation: Degradation) !void {
    std.debug.print("ðŸš¨ PERFORMANCE DEGRADATION ALERT! ðŸš¨\n", .{});

    if (config.alert_webhook) |webhook| {
        // In a real implementation, send webhook notification
        _ = webhook;
        _ = allocator;
        _ = metrics;
        _ = baseline;
        _ = degradation;
        std.debug.print("   Alert sent to webhook\n", .{});
    }
}

fn printHelp() void {
    std.debug.print(
        \\WDBX Continuous Performance Monitor
        \\
        \\Usage: continuous_monitor [options]
        \\
        \\Options:
        \\  --interval <seconds>    Check interval (default: 60)
        \\  --threshold <percent>   Alert threshold percentage (default: 10.0)
        \\  --baseline <file>       Baseline file path (default: performance_baseline.json)
        \\  --webhook <url>         Alert webhook URL
        \\  --endpoint <url>        Metrics endpoint URL (default: http://localhost:8080/stats)
        \\  --no-log               Disable logging to file
        \\  --help                 Show this help message
        \\
        \\Example:
        \\  continuous_monitor --interval 30 --threshold 5.0 --webhook https://hooks.slack.com/...
        \\
    , .{});
}
