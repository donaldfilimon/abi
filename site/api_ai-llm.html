<!DOCTYPE html>
<html lang="en" data-theme="dark">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>api_ai-llm - ABI Framework Documentation</title>
  <meta name="description" content="">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="/abi/assets/css/style.css">
</head>
<body>
<nav class="navbar">
  <div class="nav-container">
    <a href="/abi/" class="nav-logo">
      <span class="logo-text">ABI</span>
      <span class="logo-version">v0.16.0</span>
    </a>
    <div class="nav-links">
      <a href="/abi/">Home</a>
      <a href="/abi/intro.html">Docs</a>
      <a href="https://github.com/donaldfilimon/abi" target="_blank">GitHub</a>
    </div>
    <button class="theme-toggle" onclick="toggleTheme()"><span class="theme-icon">ðŸŒ™</span></button>
  </div>
</nav>
<div class="main-container">
  <main class="content" style="margin-left: 0;">
    <article class="doc-article">
      <header class="doc-header">
        <h1>api_ai-llm</h1>
      </header>
      <nav class="toc"><h3>On this page</h3><ul>
        <li style="padding-left: 0px"><a href="#ai-llm-api-reference">ai-llm API Reference</a></li>
        <li style="padding-left: 16px"><a href="#api">API</a></li>
        <li style="padding-left: 32px"><a href="#pub-const-llmerror">`pub const LlmError`</a></li>
        <li style="padding-left: 32px"><a href="#pub-const-inferenceconfig">`pub const InferenceConfig`</a></li>
        <li style="padding-left: 32px"><a href="#pub-const-inferencestats">`pub const InferenceStats`</a></li>
        <li style="padding-left: 32px"><a href="#pub-const-engine">`pub const Engine`</a></li>
        <li style="padding-left: 32px"><a href="#pub-fn-loadmodelself-engine-path-const-u8-void">`pub fn loadModel(self: *Engine, path: []const u8) !void`</a></li>
        <li style="padding-left: 32px"><a href="#pub-fn-generateself-engine-allocator-stdmemallocator-prompt-const-u8-u8">`pub fn generate(self: *Engine, allocator: std.mem.Allocator, prompt: []const u8) ![]u8`</a></li>
        <li style="padding-left: 32px"><a href="#pub-fn-generatestreaming">`pub fn generateStreaming(`</a></li>
        <li style="padding-left: 32px"><a href="#pub-fn-tokenizeself-engine-allocator-stdmemallocator-text-const-u8-u32">`pub fn tokenize(self: *Engine, allocator: std.mem.Allocator, text: []const u8) ![]u32`</a></li>
        <li style="padding-left: 32px"><a href="#pub-fn-detokenizeself-engine-allocator-stdmemallocator-tokens-const-u32-u8">`pub fn detokenize(self: *Engine, allocator: std.mem.Allocator, tokens: []const u32) ![]u8`</a></li>
        <li style="padding-left: 32px"><a href="#pub-fn-getstatsself-engine-inferencestats">`pub fn getStats(self: *Engine) InferenceStats`</a></li>
        <li style="padding-left: 32px"><a href="#pub-const-context">`pub const Context`</a></li>
        <li style="padding-left: 32px"><a href="#pub-fn-getengineself-context-engine">`pub fn getEngine(self: *Context) !*Engine`</a></li>
        <li style="padding-left: 32px"><a href="#pub-fn-generateself-context-prompt-const-u8-u8">`pub fn generate(self: *Context, prompt: []const u8) ![]u8`</a></li>
        <li style="padding-left: 32px"><a href="#pub-fn-tokenizeself-context-text-const-u8-u32">`pub fn tokenize(self: *Context, text: []const u8) ![]u32`</a></li>
        <li style="padding-left: 32px"><a href="#pub-fn-detokenizeself-context-tokens-const-u32-u8">`pub fn detokenize(self: *Context, tokens: []const u32) ![]u8`</a></li>
        <li style="padding-left: 32px"><a href="#pub-fn-isenabled-bool">`pub fn isEnabled() bool`</a></li>
        <li style="padding-left: 32px"><a href="#pub-fn-inferallocator-stdmemallocator-modelpath-const-u8-prompt-const-u8-u8">`pub fn infer(allocator: std.mem.Allocator, model_path: []const u8, prompt: []const u8) ![]u8`</a></li>
      </ul></nav>
      <div class="doc-content"><h1 id="ai-llm-api-reference">ai-llm API Reference<a class="anchor" href="#ai-llm-api-reference">#</a></h1>

<blockquote>Local LLM inference</blockquote>

<p><strong>Source:</strong> <a href="../../src/ai/llm/mod.zig"><code>src/ai/llm/mod.zig</code></a></p>

<hr>

<p>LLM Sub-module</p>

<p>Local LLM inference supporting GGUF models and transformer architectures.</p>

<p>This module provides a pure Zig implementation for loading and running</p>
<p>large language models locally without external dependencies. Supports:</p>
<li>GGUF model format (llama.cpp compatible)</li>
<li>BPE tokenization</li>
<li>Quantized inference (Q4_0, Q8_0)</li>
<li>KV caching for efficient autoregressive generation</li>
<li>GPU acceleration with CPU fallback</li>

<p>Usage:</p>
<pre class="code-block language-zig"><code>const llm = @import(&quot;llm&quot;);

var model = try llm.Model.load(allocator, &quot;model.gguf&quot;);
defer model.deinit();

var generator = model.generator(.{});
const output = try generator.generate(allocator, &quot;Hello, world!&quot;);
</code></pre>

<hr>

<h2 id="api">API<a class="anchor" href="#api">#</a></h2>

<h3 id="pub-const-llmerror"><code>pub const LlmError</code><a class="anchor" href="#pub-const-llmerror">#</a></h3>

<p>&lt;sup&gt;<strong>const</strong>&lt;/sup&gt;</p>

<p>LLM-specific errors</p>

<h3 id="pub-const-inferenceconfig"><code>pub const InferenceConfig</code><a class="anchor" href="#pub-const-inferenceconfig">#</a></h3>

<p>&lt;sup&gt;<strong>type</strong>&lt;/sup&gt;</p>

<p>Configuration for LLM inference</p>

<h3 id="pub-const-inferencestats"><code>pub const InferenceStats</code><a class="anchor" href="#pub-const-inferencestats">#</a></h3>

<p>&lt;sup&gt;<strong>type</strong>&lt;/sup&gt;</p>

<p>Statistics from inference</p>

<h3 id="pub-const-engine"><code>pub const Engine</code><a class="anchor" href="#pub-const-engine">#</a></h3>

<p>&lt;sup&gt;<strong>type</strong>&lt;/sup&gt;</p>

<p>High-level interface for loading and running models</p>

<h3 id="pub-fn-loadmodelself-engine-path-const-u8-void"><code>pub fn loadModel(self: *Engine, path: []const u8) !void</code><a class="anchor" href="#pub-fn-loadmodelself-engine-path-const-u8-void">#</a></h3>

<p>&lt;sup&gt;<strong>fn</strong>&lt;/sup&gt;</p>

<p>Load a model from a GGUF file</p>

<h3 id="pub-fn-generateself-engine-allocator-stdmemallocator-prompt-const-u8-u8"><code>pub fn generate(self: *Engine, allocator: std.mem.Allocator, prompt: []const u8) ![]u8</code><a class="anchor" href="#pub-fn-generateself-engine-allocator-stdmemallocator-prompt-const-u8-u8">#</a></h3>

<p>&lt;sup&gt;<strong>fn</strong>&lt;/sup&gt;</p>

<p>Generate text from a prompt</p>

<h3 id="pub-fn-generatestreaming"><code>pub fn generateStreaming(</code><a class="anchor" href="#pub-fn-generatestreaming">#</a></h3>

<p>&lt;sup&gt;<strong>fn</strong>&lt;/sup&gt;</p>

<p>Generate with streaming callback (per-token)</p>

<h3 id="pub-fn-tokenizeself-engine-allocator-stdmemallocator-text-const-u8-u32"><code>pub fn tokenize(self: *Engine, allocator: std.mem.Allocator, text: []const u8) ![]u32</code><a class="anchor" href="#pub-fn-tokenizeself-engine-allocator-stdmemallocator-text-const-u8-u32">#</a></h3>

<p>&lt;sup&gt;<strong>fn</strong>&lt;/sup&gt;</p>

<p>Tokenize text</p>

<h3 id="pub-fn-detokenizeself-engine-allocator-stdmemallocator-tokens-const-u32-u8"><code>pub fn detokenize(self: *Engine, allocator: std.mem.Allocator, tokens: []const u32) ![]u8</code><a class="anchor" href="#pub-fn-detokenizeself-engine-allocator-stdmemallocator-tokens-const-u32-u8">#</a></h3>

<p>&lt;sup&gt;<strong>fn</strong>&lt;/sup&gt;</p>

<p>Detokenize tokens</p>

<h3 id="pub-fn-getstatsself-engine-inferencestats"><code>pub fn getStats(self: *Engine) InferenceStats</code><a class="anchor" href="#pub-fn-getstatsself-engine-inferencestats">#</a></h3>

<p>&lt;sup&gt;<strong>fn</strong>&lt;/sup&gt;</p>

<p>Get current statistics</p>

<h3 id="pub-const-context"><code>pub const Context</code><a class="anchor" href="#pub-const-context">#</a></h3>

<p>&lt;sup&gt;<strong>type</strong>&lt;/sup&gt;</p>

<p>LLM context for framework integration.</p>

<h3 id="pub-fn-getengineself-context-engine"><code>pub fn getEngine(self: *Context) !*Engine</code><a class="anchor" href="#pub-fn-getengineself-context-engine">#</a></h3>

<p>&lt;sup&gt;<strong>fn</strong>&lt;/sup&gt;</p>

<p>Get or initialize the LLM engine.</p>

<h3 id="pub-fn-generateself-context-prompt-const-u8-u8"><code>pub fn generate(self: *Context, prompt: []const u8) ![]u8</code><a class="anchor" href="#pub-fn-generateself-context-prompt-const-u8-u8">#</a></h3>

<p>&lt;sup&gt;<strong>fn</strong>&lt;/sup&gt;</p>

<p>Generate text from prompt.</p>

<h3 id="pub-fn-tokenizeself-context-text-const-u8-u32"><code>pub fn tokenize(self: *Context, text: []const u8) ![]u32</code><a class="anchor" href="#pub-fn-tokenizeself-context-text-const-u8-u32">#</a></h3>

<p>&lt;sup&gt;<strong>fn</strong>&lt;/sup&gt;</p>

<p>Tokenize text.</p>

<h3 id="pub-fn-detokenizeself-context-tokens-const-u32-u8"><code>pub fn detokenize(self: *Context, tokens: []const u32) ![]u8</code><a class="anchor" href="#pub-fn-detokenizeself-context-tokens-const-u32-u8">#</a></h3>

<p>&lt;sup&gt;<strong>fn</strong>&lt;/sup&gt;</p>

<p>Detokenize tokens.</p>

<h3 id="pub-fn-isenabled-bool"><code>pub fn isEnabled() bool</code><a class="anchor" href="#pub-fn-isenabled-bool">#</a></h3>

<p>&lt;sup&gt;<strong>fn</strong>&lt;/sup&gt;</p>

<p>Check if LLM features are enabled</p>

<h3 id="pub-fn-inferallocator-stdmemallocator-modelpath-const-u8-prompt-const-u8-u8"><code>pub fn infer(allocator: std.mem.Allocator, model_path: []const u8, prompt: []const u8) ![]u8</code><a class="anchor" href="#pub-fn-inferallocator-stdmemallocator-modelpath-const-u8-prompt-const-u8-u8">#</a></h3>

<p>&lt;sup&gt;<strong>fn</strong>&lt;/sup&gt;</p>

<p>Quick inference helper</p>

<hr>

<p>*Generated automatically by <code>zig build gendocs</code>*</p>
</div>
    </article>
  </main>
</div>
<footer class="footer" style="margin-left: 0;">
  <div class="footer-content">
    <div class="footer-section">
      <h4>ABI Framework</h4>
      <p>Modern Zig framework for AI services and high-performance systems.</p>
    </div>
  </div>
  <div class="footer-bottom"><p>&copy; 2026 ABI Framework. Built with Zig.</p></div>
</footer>
<script src="/abi/assets/js/main.js"></script>
</body>
</html>
