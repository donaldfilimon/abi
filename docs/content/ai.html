<p>
The AI module includes agents, local LLM inference, streaming responses, and
training pipelines. It is feature gated and can be compiled out for minimal
builds.
</p>

<h2>Capabilities</h2>
<ul>
    <li>Local GGUF inference with streaming tokens</li>
    <li>Agent personas and tool usage workflows</li>
    <li>Model management and capability discovery</li>
    <li>Training pipelines for LLM and vision tasks</li>
</ul>

<h2>Enable and configure</h2>
<pre><code>zig build -Denable-ai=true -Denable-llm=true
export ABI_LLM_MODEL_PATH=/models/llama.gguf
</code></pre>

<h2>Streaming inference</h2>
<p>
The streaming server exposes SSE and WebSocket endpoints. Use the CLI to run a
server and point your client to the chosen port.
</p>
<pre><code>zig build run -- llm serve -m ./models/llama.gguf
</code></pre>
