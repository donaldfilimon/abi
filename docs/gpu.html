<!DOCTYPE html>
<html lang="en" data-theme="dark">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>gpu - ABI Framework Documentation</title>
  <meta name="description" content="">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="/abi/assets/css/style.css">
</head>
<body>
<nav class="navbar">
  <div class="nav-container">
    <a href="/abi/" class="nav-logo">
      <span class="logo-text">ABI</span>
      <span class="logo-version">v0.16.0</span>
    </a>
    <div class="nav-links">
      <a href="/abi/">Home</a>
      <a href="/abi/intro.html">Docs</a>
      <a href="https://github.com/donaldfilimon/abi" target="_blank">GitHub</a>
    </div>
    <button class="theme-toggle" onclick="toggleTheme()"><span class="theme-icon">üåô</span></button>
  </div>
</nav>
<div class="main-container">
  <main class="content" style="margin-left: 0;">
    <article class="doc-article">
      <header class="doc-header">
        <h1>gpu</h1>
      </header>
      <nav class="toc"><h3>On this page</h3><ul>
        <li style="padding-left: 0px"><a href="#gpu-acceleration">GPU Acceleration</a></li>
        <li style="padding-left: 16px"><a href="#feature-overview">Feature Overview</a></li>
        <li style="padding-left: 16px"><a href="#unified-gpu-api-recommended">Unified GPU API (Recommended)</a></li>
        <li style="padding-left: 32px"><a href="#quick-start">Quick Start</a></li>
        <li style="padding-left: 32px"><a href="#configuration-options">Configuration Options</a></li>
        <li style="padding-left: 32px"><a href="#high-level-operations">High-Level Operations</a></li>
        <li style="padding-left: 32px"><a href="#multi-gpu-support">Multi-GPU Support</a></li>
        <li style="padding-left: 32px"><a href="#profiling-and-metrics">Profiling and Metrics</a></li>
        <li style="padding-left: 32px"><a href="#device-enumeration">Device Enumeration</a></li>
        <li style="padding-left: 32px"><a href="#backend-auto-detection">Backend Auto-Detection</a></li>
        <li style="padding-left: 32px"><a href="#unified-execution-coordinator">Unified Execution Coordinator</a></li>
        <li style="padding-left: 16px"><a href="#portable-kernel-dsl">Portable Kernel DSL</a></li>
        <li style="padding-left: 32px"><a href="#building-a-kernel">Building a Kernel</a></li>
        <li style="padding-left: 32px"><a href="#code-generation-targets">Code Generation Targets</a></li>
        <li style="padding-left: 16px"><a href="#backends">Backends</a></li>
        <li style="padding-left: 32px"><a href="#backend-details">Backend Details</a></li>
        <li style="padding-left: 16px"><a href="#fpga-backend-roadmap">FPGA Backend Roadmap</a></li>
        <li style="padding-left: 32px"><a href="#current-status">Current Status</a></li>
        <li style="padding-left: 32px"><a href="#development-phases">Development Phases</a></li>
        <li style="padding-left: 32px"><a href="#target-use-cases">Target Use Cases</a></li>
        <li style="padding-left: 32px"><a href="#related-documentation">Related Documentation</a></li>
        <li style="padding-left: 16px"><a href="#memory-management">Memory Management</a></li>
        <li style="padding-left: 32px"><a href="#smart-buffers-unified-api">Smart Buffers (Unified API)</a></li>
        <li style="padding-left: 32px"><a href="#legacy-memory-pool">Legacy Memory Pool</a></li>
        <li style="padding-left: 16px"><a href="#cli-commands">CLI Commands</a></li>
        <li style="padding-left: 16px"><a href="#building-with-gpu-support">Building with GPU Support</a></li>
        <li style="padding-left: 16px"><a href="#new-in-202601">New in 2026.01</a></li>
        <li style="padding-left: 32px"><a href="#diagnostics">Diagnostics</a></li>
        <li style="padding-left: 32px"><a href="#error-context">Error Context</a></li>
        <li style="padding-left: 32px"><a href="#graceful-degradation">Graceful Degradation</a></li>
        <li style="padding-left: 32px"><a href="#simd-cpu-fallback">SIMD CPU Fallback</a></li>
        <li style="padding-left: 16px"><a href="#api-reference">API Reference</a></li>
        <li style="padding-left: 32px"><a href="#types">Types</a></li>
        <li style="padding-left: 32px"><a href="#gpu-methods">Gpu Methods</a></li>
        <li style="padding-left: 32px"><a href="#executionresult-methods">ExecutionResult Methods</a></li>
        <li style="padding-left: 16px"><a href="#see-also">See Also</a></li>
        <li style="padding-left: 32px"><a href="#related-guides">Related Guides</a></li>
        <li style="padding-left: 32px"><a href="#resources">Resources</a></li>
      </ul></nav>
      <div class="doc-content"><h1 id="gpu-acceleration">GPU Acceleration<a class="anchor" href="#gpu-acceleration">#</a></h1>
<blockquote><strong>Codebase Status:</strong> Synced with repository as of 2026-01-23.</blockquote>

<p>&lt;p align=&quot;center&quot;&gt;</p>
<p>  &lt;img src=&quot;https://img.shields.io/badge/Module-GPU-green?style=for-the-badge&amp;logo=nvidia&amp;logoColor=white&quot; alt=&quot;GPU Module&quot;/&gt;</p>
<p>  &lt;img src=&quot;https://img.shields.io/badge/Status-Production_Ready-success?style=for-the-badge&quot; alt=&quot;Production Ready&quot;/&gt;</p>
<p>  &lt;img src=&quot;https://img.shields.io/badge/Backends-8-blue?style=for-the-badge&quot; alt=&quot;8 Backends&quot;/&gt;</p>
<p>&lt;/p&gt;</p>

<p>&lt;p align=&quot;center&quot;&gt;</p>
<p>  &lt;img src=&quot;https://img.shields.io/badge/CUDA-Supported-76B900?logo=nvidia&amp;logoColor=white&quot; alt=&quot;CUDA&quot;/&gt;</p>
<p>  &lt;img src=&quot;https://img.shields.io/badge/Vulkan-Supported-AC162C?logo=vulkan&amp;logoColor=white&quot; alt=&quot;Vulkan&quot;/&gt;</p>
<p>  &lt;img src=&quot;https://img.shields.io/badge/Metal-Supported-000000?logo=apple&amp;logoColor=white&quot; alt=&quot;Metal&quot;/&gt;</p>
<p>  &lt;img src=&quot;https://img.shields.io/badge/WebGPU-Supported-005A9C?logo=w3c&amp;logoColor=white&quot; alt=&quot;WebGPU&quot;/&gt;</p>
<p>&lt;/p&gt;</p>

<p>&lt;p align=&quot;center&quot;&gt;</p>
<p>  &lt;a href=&quot;#unified-gpu-api-recommended&quot;&gt;Unified API&lt;/a&gt; ‚Ä¢</p>
<p>  &lt;a href=&quot;#backends&quot;&gt;Backends&lt;/a&gt; ‚Ä¢</p>
<p>  &lt;a href=&quot;#portable-kernel-dsl&quot;&gt;Kernel DSL&lt;/a&gt; ‚Ä¢</p>
<p>  &lt;a href=&quot;#device-enumeration&quot;&gt;Device Enum&lt;/a&gt; ‚Ä¢</p>
<p>  &lt;a href=&quot;#cli-commands&quot;&gt;CLI&lt;/a&gt;</p>
<p>&lt;/p&gt;</p>

<hr>

<blockquote><strong>Status</strong>: Production Ready. Backends provide native GPU execution with automatic fallback to CPU simulation when native runtimes are unavailable.</blockquote>

<blockquote><strong>Developer Guide</strong>: See <a href="../CONTRIBUTING.md">CONTRIBUTING.md</a> for GPU coding patterns and <a href="../CLAUDE.md">CLAUDE.md</a> for backend internals.</blockquote>
<blockquote><strong>GPU Backends</strong>: See <a href="gpu-backend-improvements.md">GPU Backend Details</a> for implementation specifics.</blockquote>

<p>The <strong>GPU</strong> module (<code>abi.gpu</code>) provides a unified interface for hardware-accelerated compute across different platforms.</p>

<h2 id="feature-overview">Feature Overview<a class="anchor" href="#feature-overview">#</a></h2>

<tr><td>Feature</td><td>Description</td><td>Status</td></tr>
<tr><td><strong>Unified API</strong></td><td>Single interface for all backends</td><td>!<a href="https://img.shields.io/badge/-Ready-success">Ready</a></td></tr>
<tr><td><strong>Multi-GPU</strong></td><td>Multi-device load balancing</td><td>!<a href="https://img.shields.io/badge/-Ready-success">Ready</a></td></tr>
<tr><td><strong>Auto-Detection</strong></td><td>Backend selection with fallback</td><td>!<a href="https://img.shields.io/badge/-Ready-success">Ready</a></td></tr>
<tr><td><strong>Kernel DSL</strong></td><td>Write once, compile everywhere</td><td>!<a href="https://img.shields.io/badge/-Ready-success">Ready</a></td></tr>
<tr><td><strong>SIMD Fallback</strong></td><td>AVX/SSE/NEON when no GPU</td><td>!<a href="https://img.shields.io/badge/-Ready-success">Ready</a></td></tr>
<tr><td><strong>Profiling</strong></td><td>Per-kernel metrics &amp; timing</td><td>!<a href="https://img.shields.io/badge/-Ready-success">Ready</a></td></tr>

<h2 id="unified-gpu-api-recommended">Unified GPU API (Recommended)<a class="anchor" href="#unified-gpu-api-recommended">#</a></h2>

<p>The new unified GPU API provides a single interface for all 8 backends with smart buffer management and optional profiling.</p>

<h3 id="quick-start">Quick Start<a class="anchor" href="#quick-start">#</a></h3>

<pre class="code-block language-zig"><code>const abi = @import(&quot;abi&quot;);

pub fn main() !void {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    // Initialize unified GPU API
    var gpu = try abi.Gpu.init(allocator, .{
        .enable_profiling = true,
        .memory_mode = .automatic,
    });
    defer gpu.deinit();

    // Create buffers with automatic memory management
    const a = try gpu.createBufferFromSlice(f32, &amp;[_]f32{ 1, 2, 3, 4 }, .{});
    defer gpu.destroyBuffer(a);

    const b = try gpu.createBufferFromSlice(f32, &amp;[_]f32{ 5, 6, 7, 8 }, .{});
    defer gpu.destroyBuffer(b);

    const result = try gpu.createBuffer(4 * @sizeOf(f32), .{});
    defer gpu.destroyBuffer(result);

    // Execute vector addition (transfers handled automatically)
    _ = try gpu.vectorAdd(a, b, result);

    // Read results back
    var output: [4]f32 = undefined;
    try result.read(f32, &amp;output);
    // output = { 6, 8, 10, 12 }
}
</code></pre>

<h3 id="configuration-options">Configuration Options<a class="anchor" href="#configuration-options">#</a></h3>

<pre class="code-block language-zig"><code>pub const GpuConfig = struct {
    preferred_backend: ?Backend = null,    // null = auto-select best
    allow_fallback: bool = true,
    memory_mode: MemoryMode = .automatic,
    max_memory_bytes: usize = 0,           // 0 = unlimited
    enable_profiling: bool = false,
    multi_gpu: bool = false,
    load_balance_strategy: LoadBalanceStrategy = .memory_aware,
};

pub const MemoryMode = enum {
    automatic,  // API handles all transfers (recommended)
    explicit,   // User controls transfers via toDevice()/toHost()
    unified,    // Use unified memory where available
};
</code></pre>

<h3 id="high-level-operations">High-Level Operations<a class="anchor" href="#high-level-operations">#</a></h3>

<p>The unified API provides these built-in operations:</p>

<tr><td>Operation</td><td>Description</td></tr>
<tr><td><code>vectorAdd(a, b, result)</code></td><td>Element-wise vector addition</td></tr>
<tr><td><code>matrixMultiply(a, b, result, dims)</code></td><td>Matrix multiplication</td></tr>
<tr><td><code>reduceSum(input)</code></td><td>Sum reduction</td></tr>
<tr><td><code>dotProduct(a, b)</code></td><td>Dot product of two vectors</td></tr>
<tr><td><code>softmax(input, output)</code></td><td>Softmax activation</td></tr>

<h3 id="multi-gpu-support">Multi-GPU Support<a class="anchor" href="#multi-gpu-support">#</a></h3>

<pre class="code-block language-zig"><code>var gpu = try abi.Gpu.init(allocator, .{
    .multi_gpu = true,
    .load_balance_strategy = .memory_aware,
});

// Enable multi-GPU after init
try gpu.enableMultiGpu(.{
    .strategy = .round_robin,
    .enable_peer_access = true,
});

// Get multi-GPU stats
if (gpu.getMultiGpuStats()) |stats| {
    std.debug.print(&quot;Active devices: {}\n&quot;, .{stats.active_device_count});
}
</code></pre>

<h3 id="profiling-and-metrics">Profiling and Metrics<a class="anchor" href="#profiling-and-metrics">#</a></h3>

<pre class="code-block language-zig"><code>var gpu = try abi.Gpu.init(allocator, .{
    .enable_profiling = true,
});

// ... execute operations ...

// Get metrics summary
if (gpu.getMetricsSummary()) |summary| {
    std.debug.print(&quot;Total kernel invocations: {d}\n&quot;, .{summary.total_kernel_invocations});
    std.debug.print(&quot;Average kernel time: {d:.3}ns\n&quot;, .{summary.avg_kernel_time_ns});
}

// Get per-kernel metrics
if (gpu.getKernelMetrics(&quot;vectorAdd&quot;)) |metrics| {
    std.debug.print(&quot;vectorAdd invocations: {d}\n&quot;, .{metrics.invocation_count});
}
</code></pre>

<h3 id="device-enumeration">Device Enumeration<a class="anchor" href="#device-enumeration">#</a></h3>

<p>Enumerate all available GPU devices across all backends:</p>

<pre class="code-block language-zig"><code>const device = abi.gpu.device;

// Enumerate all devices
const all_devices = try device.enumerateAllDevices(allocator);
defer allocator.free(all_devices);

for (all_devices) |dev| {
    std.debug.print(&quot;Device: {s} ({})\n&quot;, .{dev.name, dev.backend});
    std.debug.print(&quot;  Type: {}\n&quot;, .{dev.device_type});
    if (dev.total_memory) |mem| {
        std.debug.print(&quot;  Memory: {} GB\n&quot;, .{mem / (1024 * 1024 * 1024)});
    }
}

// Enumerate devices for a specific backend
const cuda_devices = try device.enumerateDevicesForBackend(allocator, .cuda);
defer allocator.free(cuda_devices);

// Select best device with criteria
const criteria = device.DeviceSelectionCriteria{
    .prefer_discrete = true,
    .min_memory_gb = 4,
    .required_features = &amp;.{.fp16},
};
const best = try device.selectBestDevice(allocator, criteria);
</code></pre>

<h3 id="backend-auto-detection">Backend Auto-Detection<a class="anchor" href="#backend-auto-detection">#</a></h3>

<p>The backend factory provides enhanced auto-detection with fallback chains:</p>

<pre class="code-block language-zig"><code>const backend_factory = abi.gpu.backend_factory;

// Detect all available backends
const backends = try backend_factory.detectAvailableBackends(allocator);
defer allocator.free(backends);

// Select best backend with fallback chain
const best = try backend_factory.selectBestBackendWithFallback(allocator, .{
    .preferred = .cuda,
    .fallback_chain = &amp;.{ .vulkan, .metal, .stdgpu },
});

// Select backend with specific feature requirements
const fp16_backend = try backend_factory.selectBackendWithFeatures(allocator, .{
    .required_features = &amp;.{.fp16, .atomics},
    .fallback_to_cpu = false,
});
</code></pre>

<h3 id="unified-execution-coordinator">Unified Execution Coordinator<a class="anchor" href="#unified-execution-coordinator">#</a></h3>

<p>Automatic GPU ‚Üí SIMD ‚Üí scalar fallback for optimal performance:</p>

<pre class="code-block language-zig"><code>const exec = abi.gpu.execution_coordinator;

var coordinator = try exec.ExecutionCoordinator.init(allocator, .{
    .prefer_gpu = true,
    .fallback_chain = &amp;.{ .gpu, .simd, .scalar },
    .gpu_threshold_size = 1024,  // Min elements for GPU
    .simd_threshold_size = 4,    // Min elements for SIMD
});
defer coordinator.deinit();

// Automatic method selection
const a = [_]f32{ 1, 2, 3, 4, 5, 6, 7, 8 };
const b = [_]f32{ 8, 7, 6, 5, 4, 3, 2, 1 };
var result = [_]f32{0} ** 8;

const method = try coordinator.vectorAdd(&amp;a, &amp;b, &amp;result);
// method is .gpu, .simd, or .scalar depending on availability and size

// Explicit method override
const forced_method = try coordinator.vectorAddWithMethod(&amp;a, &amp;b, &amp;result, .simd);
</code></pre>

<h2 id="portable-kernel-dsl">Portable Kernel DSL<a class="anchor" href="#portable-kernel-dsl">#</a></h2>

<p>Write kernels once, compile to all backends (CUDA, GLSL, WGSL, MSL).</p>

<h3 id="building-a-kernel">Building a Kernel<a class="anchor" href="#building-a-kernel">#</a></h3>

<pre class="code-block language-zig"><code>const dsl = abi.gpu.dsl;

var builder = dsl.KernelBuilder.init(allocator, &quot;scale_vector&quot;);
defer builder.deinit();

// Set workgroup size
_ = builder.setWorkgroupSize(256, 1, 1);

// Define bindings
const input = try builder.addBuffer(&quot;input&quot;, 0, .{ .scalar = .f32 }, .read_only);
const output = try builder.addBuffer(&quot;output&quot;, 1, .{ .scalar = .f32 }, .write_only);
const scale = try builder.addUniform(&quot;scale&quot;, 2, .{ .scalar = .f32 });

// Get global invocation ID
const gid = builder.globalInvocationId();
const idx = try builder.component(try gid.toExpr(), &quot;x&quot;);

// output[idx] = input[idx] * scale
const scaled = try builder.mul(
    try builder.index(try input.toExpr(), idx),
    try scale.toExpr()
);
try builder.addStatement(try builder.assignStmt(
    try builder.index(try output.toExpr(), idx),
    scaled
));

// Build and compile
const ir = try builder.build();
var kernel = try gpu.compileKernel(.{ .ir = &amp;ir });
defer kernel.deinit();
</code></pre>

<h3 id="code-generation-targets">Code Generation Targets<a class="anchor" href="#code-generation-targets">#</a></h3>

<tr><td>IR Construct</td><td>CUDA</td><td>GLSL</td><td>WGSL</td><td>MSL</td></tr>
<tr><td><code>global_id</code></td><td><code>blockIdx.x * blockDim.x + threadIdx.x</code></td><td><code>gl_GlobalInvocationID.x</code></td><td><code>@builtin(global_invocation_id)</code></td><td><code>thread_position_in_grid</code></td></tr>
<tr><td><code>barrier()</code></td><td><code>__syncthreads()</code></td><td><code>barrier()</code></td><td><code>workgroupBarrier()</code></td><td><code>threadgroup_barrier()</code></td></tr>
<tr><td><code>atomic_add</code></td><td><code>atomicAdd()</code></td><td><code>atomicAdd()</code></td><td><code>atomicAdd()</code></td><td><code>atomic_fetch_add_explicit()</code></td></tr>
<tr><td><code>buffer&lt;f32&gt;</code></td><td><code>float*</code></td><td><code>buffer { float data[]; }</code></td><td><code>var&lt;storage&gt; array&lt;f32&gt;</code></td><td><code>device float*</code></td></tr>

<h2 id="backends">Backends<a class="anchor" href="#backends">#</a></h2>

<p>ABI supports 8 GPU backends with comprehensive implementations:</p>

<tr><td>Backend</td><td>Platform</td><td>Features</td><td>Status</td></tr>
<tr><td><strong>CUDA</strong></td><td>NVIDIA GPUs</td><td>Tensor cores, async D2D, device queries</td><td>!<a href="https://img.shields.io/badge/-Complete-success">Complete</a></td></tr>
<tr><td><strong>Vulkan</strong></td><td>Cross-platform</td><td>SPIR-V generation, compute shaders</td><td>!<a href="https://img.shields.io/badge/-Complete-success">Complete</a></td></tr>
<tr><td><strong>Metal</strong></td><td>Apple Silicon</td><td>Objective-C bindings, compute kernels</td><td>!<a href="https://img.shields.io/badge/-Complete-success">Complete</a></td></tr>
<tr><td><strong>WebGPU</strong></td><td>Browser/Native</td><td>Async handling, cross-platform</td><td>!<a href="https://img.shields.io/badge/-Complete-success">Complete</a></td></tr>
<tr><td><strong>OpenGL/ES</strong></td><td>Legacy/Mobile</td><td>Compute shaders (4.3+/ES 3.1+)</td><td>!<a href="https://img.shields.io/badge/-Complete-success">Complete</a></td></tr>
<tr><td><strong>std.gpu</strong></td><td>Zig stdlib</td><td>CPU fallback, portable</td><td>!<a href="https://img.shields.io/badge/-Complete-success">Complete</a></td></tr>
<tr><td><strong>OpenCL</strong></td><td>Cross-platform</td><td>Legacy compute support</td><td>!<a href="https://img.shields.io/badge/-Complete-success">Complete</a></td></tr>
<tr><td><strong>WebGL2</strong></td><td>Browser</td><td>Rendering only (no compute)</td><td>!<a href="https://img.shields.io/badge/-Limited-yellow">Limited</a></td></tr>
<tr><td><strong>FPGA</strong></td><td>Data center/Edge</td><td>AMD/Xilinx, Intel FPGAs</td><td>!<a href="https://img.shields.io/badge/-Development-orange">Development</a></td></tr>

<h3 id="backend-details">Backend Details<a class="anchor" href="#backend-details">#</a></h3>

<p>&lt;details&gt;</p>
<p>&lt;summary&gt;&lt;strong&gt;CUDA (NVIDIA GPUs)&lt;/strong&gt;&lt;/summary&gt;</p>

<li><strong>Platform</strong>: Linux/Windows</li>
<li><strong>Features</strong>: Tensor core support, async D2D transfers, full device queries</li>
<li><strong>Best for</strong>: High-performance compute, ML training</li>
<p>&lt;/details&gt;</p>

<p>&lt;details&gt;</p>
<p>&lt;summary&gt;&lt;strong&gt;Vulkan (Cross-platform)&lt;/strong&gt;&lt;/summary&gt;</p>

<li><strong>Platform</strong>: Linux/Windows/Android</li>
<li><strong>Features</strong>: SPIR-V shader generation, compute shaders</li>
<li><strong>Best for</strong>: Cross-platform GPU compute</li>
<p>&lt;/details&gt;</p>

<p>&lt;details&gt;</p>
<p>&lt;summary&gt;&lt;strong&gt;Metal (Apple Silicon)&lt;/strong&gt;&lt;/summary&gt;</p>

<li><strong>Platform</strong>: macOS/iOS</li>
<li><strong>Features</strong>: Objective-C runtime bindings, compute kernels</li>
<li><strong>Best for</strong>: Apple hardware optimization</li>
<p>&lt;/details&gt;</p>

<p>&lt;details&gt;</p>
<p>&lt;summary&gt;&lt;strong&gt;WebGPU (Browser/Native)&lt;/strong&gt;&lt;/summary&gt;</p>

<li><strong>Platform</strong>: Browser (via Dawn/wgpu)</li>
<li><strong>Features</strong>: Async adapter handling, portable compute</li>
<li><strong>Best for</strong>: Web-based GPU compute</li>
<p>&lt;/details&gt;</p>

<p>&lt;details&gt;</p>
<p>&lt;summary&gt;&lt;strong&gt;std.gpu (Zig stdlib)&lt;/strong&gt;&lt;/summary&gt;</p>

<li><strong>Platform</strong>: Any</li>
<li><strong>Features</strong>: Automatic CPU fallback, SIMD acceleration</li>
<li><strong>Best for</strong>: Development, testing, CPU-only environments</li>
<p>&lt;/details&gt;</p>

<p>&lt;details&gt;</p>
<p>&lt;summary&gt;&lt;strong&gt;FPGA (AMD/Xilinx, Intel)&lt;/strong&gt;&lt;/summary&gt;</p>

<li><strong>Platform</strong>: Data center (Alveo), Edge (Versal)</li>
<li><strong>Features</strong>: Quantized matmul, vector search, deterministic latency</li>
<li><strong>Best for</strong>: Power-efficient inference, real-time systems</li>
<li><strong>Status</strong>: Development phase</li>
<li><strong>Documentation</strong>: See <a href="../src/gpu/backends/fpga/README.md">FPGA Backend README</a></li>
<p>&lt;/details&gt;</p>

<h2 id="fpga-backend-roadmap">FPGA Backend Roadmap<a class="anchor" href="#fpga-backend-roadmap">#</a></h2>

<p>The FPGA backend is under active development, targeting hardware acceleration for specific workloads where FPGAs excel.</p>

<h3 id="current-status">Current Status<a class="anchor" href="#current-status">#</a></h3>

<tr><td>Component</td><td>Status</td><td>Description</td></tr>
<tr><td>Backend Interface</td><td>Complete</td><td>VTable implementation for GPU interface</td></tr>
<tr><td>Memory Management</td><td>Complete</td><td>DDR/HBM allocation and transfer</td></tr>
<tr><td>Bitstream Loading</td><td>Complete</td><td>AMD/Xilinx XRT integration (stub)</td></tr>
<tr><td>Vector Distance</td><td>Implemented</td><td>Cosine, L2, dot product</td></tr>
<tr><td>Quantized MatMul</td><td>Implemented</td><td>Q4, Q8 matrix operations</td></tr>
<tr><td>K-means</td><td>Implemented</td><td>Centroid assignment</td></tr>
<tr><td>Softmax</td><td>Implemented</td><td>Streaming implementation</td></tr>

<h3 id="development-phases">Development Phases<a class="anchor" href="#development-phases">#</a></h3>

<p><strong>Phase 1: Foundation (Q1 2026)</strong> - Complete</p>
<li>Backend structure and interface</li>
<li>Memory management</li>
<li>CPU simulation for testing</li>

<p><strong>Phase 2: Core Kernels (Q2-Q3 2026)</strong> - In Progress</p>
<li>HLS kernel development (Vitis)</li>
<li>Optimized vector operations</li>
<li>LLM inference primitives</li>

<p><strong>Phase 3: Production (Q4 2026)</strong></p>
<li>Intel oneAPI support</li>
<li>Multi-FPGA scaling</li>
<li>Performance validation</li>

<h3 id="target-use-cases">Target Use Cases<a class="anchor" href="#target-use-cases">#</a></h3>

<tr><td>Use Case</td><td>FPGA Advantage</td><td>Status</td></tr>
<tr><td>Vector similarity search</td><td>10-50x speedup, low latency</td><td>Development</td></tr>
<tr><td>Quantized LLM inference</td><td>Native Q4/Q8, power efficient</td><td>Planned</td></tr>
<tr><td>Real-time RAG</td><td>Deterministic &lt;1ms latency</td><td>Planned</td></tr>
<tr><td>Edge deployment</td><td>&lt;25W power budget</td><td>Planned</td></tr>

<h3 id="related-documentation">Related Documentation<a class="anchor" href="#related-documentation">#</a></h3>

<li><a href="research/hardware-acceleration-fpga-asic.md">FPGA/ASIC Research Overview</a></li>
<li><a href="research/fpga-inference-acceleration.md">FPGA for LLM Inference</a></li>
<li><a href="research/custom-asic-considerations.md">Custom ASIC Considerations</a></li>
<li><a href="research/hybrid-gpu-fpga-architecture.md">Hybrid GPU-FPGA Architecture</a></li>

<h2 id="memory-management">Memory Management<a class="anchor" href="#memory-management">#</a></h2>

<h3 id="smart-buffers-unified-api">Smart Buffers (Unified API)<a class="anchor" href="#smart-buffers-unified-api">#</a></h3>

<p>The unified API handles memory automatically by default:</p>

<pre class="code-block language-zig"><code>// Automatic mode (default) - transfers handled for you
var buf = try gpu.createBufferFromSlice(f32, &amp;data, .{});
_ = try gpu.vectorAdd(buf, other, result);  // Auto upload
try result.read(f32, &amp;output);               // Auto download

// Explicit mode - you control transfers
var buf = try gpu.createBuffer(size, .{ .mode = .explicit });
try buf.write(f32, &amp;data);
try buf.toDevice();   // Explicit upload
// ... operations ...
try buf.toHost();     // Explicit download
</code></pre>

<h3 id="legacy-memory-pool">Legacy Memory Pool<a class="anchor" href="#legacy-memory-pool">#</a></h3>

<p>Use <code>abi.gpu.GPUMemoryPool</code> for manual device memory management:</p>

<pre class="code-block language-zig"><code>var pool = abi.gpu.GPUMemoryPool.init(allocator, 1024 * 1024 * 64); // 64MB
defer pool.deinit();

const buffer = try abi.gpu.GPUBuffer.init(
    allocator,
    1024 * 1024, // 1MB
    .{ .device_local = true, .write_only = true }
);
defer buffer.deinit();
</code></pre>

<h2 id="cli-commands">CLI Commands<a class="anchor" href="#cli-commands">#</a></h2>

<p>Check GPU status and capabilities:</p>

<pre class="code-block language-bash"><code># List available backends and their status
zig build run -- gpu backends

# Show GPU module summary
zig build run -- gpu summary

# List detected GPU devices (shows native vs fallback mode)
zig build run -- gpu devices

# Show default GPU device
zig build run -- gpu default
</code></pre>

<h2 id="building-with-gpu-support">Building with GPU Support<a class="anchor" href="#building-with-gpu-support">#</a></h2>

<p>Enable GPU backends at build time:</p>

<pre class="code-block language-bash"><code># Enable all GPU backends (default)
zig build -Denable-gpu=true

# Enable only CUDA
zig build -Denable-gpu=true -Dgpu-cuda=true -Dgpu-vulkan=false -Dgpu-metal=false

# Disable GPU entirely
zig build -Denable-gpu=false
</code></pre>

<hr>

<h2 id="new-in-202601">New in 2026.01<a class="anchor" href="#new-in-202601">#</a></h2>

<h3 id="diagnostics">Diagnostics<a class="anchor" href="#diagnostics">#</a></h3>

<pre class="code-block language-zig"><code>const diag = gpu_mod.DiagnosticsInfo.collect(allocator);
if (!diag.isHealthy()) { diag.log(); }
</code></pre>

<p>Fields: <code>backend_type</code>, <code>device_count</code>, <code>memory_stats</code>, <code>kernel_cache_stats</code>, <code>is_degraded</code></p>

<h3 id="error-context">Error Context<a class="anchor" href="#error-context">#</a></h3>

<pre class="code-block language-zig"><code>const ctx = error_handling.ErrorContext.init(.backend_error, .cuda, &quot;message&quot;);
ctx.log();  // Or ctx.reportErrorFull(allocator)
</code></pre>

<h3 id="graceful-degradation">Graceful Degradation<a class="anchor" href="#graceful-degradation">#</a></h3>

<pre class="code-block language-zig"><code>var manager = failover.FailoverManager.init(allocator);
manager.setDegradationMode(.automatic);  // .none, .warn_and_continue, .silent
if (manager.isDegraded()) { /* CPU fallback active */ }
</code></pre>

<h3 id="simd-cpu-fallback">SIMD CPU Fallback<a class="anchor" href="#simd-cpu-fallback">#</a></h3>

<p>When GPU unavailable, <code>stdgpu</code> provides AVX/SSE/NEON accelerated operations:</p>
<p><code>simdVectorAdd</code>, <code>simdDotProduct</code>, <code>simdSum</code>, <code>simdRelu</code>, <code>simdSoftmax</code>, <code>simdMatVecMul</code></p>

<hr>

<h2 id="api-reference">API Reference<a class="anchor" href="#api-reference">#</a></h2>

<p><strong>Source:</strong> <code>src/gpu/unified.zig</code></p>

<h3 id="types">Types<a class="anchor" href="#types">#</a></h3>

<tr><td>Type</td><td>Description</td></tr>
<tr><td><code>Gpu</code></td><td>Main unified GPU API</td></tr>
<tr><td><code>GpuConfig</code></td><td>GPU configuration</td></tr>
<tr><td><code>ExecutionResult</code></td><td>Execution result with timing and statistics</td></tr>
<tr><td><code>MatrixDims</code></td><td>Matrix dimensions for matrix operations</td></tr>
<tr><td><code>LaunchConfig</code></td><td>Kernel launch configuration</td></tr>
<tr><td><code>CompiledKernel</code></td><td>Compiled kernel handle</td></tr>
<tr><td><code>MemoryInfo</code></td><td>GPU memory information</td></tr>
<tr><td><code>GpuStats</code></td><td>GPU statistics</td></tr>
<tr><td><code>HealthStatus</code></td><td>Health status</td></tr>
<tr><td><code>MultiGpuConfig</code></td><td>Multi-GPU configuration</td></tr>
<tr><td><code>LoadBalanceStrategy</code></td><td>Load balance strategy for multi-GPU</td></tr>

<h3 id="gpu-methods">Gpu Methods<a class="anchor" href="#gpu-methods">#</a></h3>

<tr><td>Method</td><td>Description</td></tr>
<tr><td><code>init(allocator, config)</code></td><td>Initialize the unified GPU API</td></tr>
<tr><td><code>deinit()</code></td><td>Deinitialize and cleanup</td></tr>
<tr><td><code>selectDevice(selector)</code></td><td>Select a device based on criteria</td></tr>
<tr><td><code>getActiveDevice()</code></td><td>Get the currently active device</td></tr>
<tr><td><code>listDevices()</code></td><td>List all available devices</td></tr>
<tr><td><code>enableMultiGpu(config)</code></td><td>Enable multi-GPU mode</td></tr>
<tr><td><code>getDeviceGroup()</code></td><td>Get multi-GPU device group (if enabled)</td></tr>
<tr><td><code>distributeWork(total_work)</code></td><td>Distribute work across multiple GPUs</td></tr>
<tr><td><code>createBuffer(size, options)</code></td><td>Create a new buffer</td></tr>
<tr><td><code>createBufferFromSlice(T, slice, options)</code></td><td>Create a buffer from a typed slice</td></tr>
<tr><td><code>destroyBuffer(buffer)</code></td><td>Destroy a buffer</td></tr>
<tr><td><code>vectorAdd(a, b, result)</code></td><td>Vector addition: result = a + b</td></tr>
<tr><td><code>matrixMultiply(a, b, result, dims)</code></td><td>Matrix multiplication: result = a * b</td></tr>
<tr><td><code>reduceSum(input)</code></td><td>Reduce sum: returns sum of all elements</td></tr>
<tr><td><code>dotProduct(a, b)</code></td><td>Dot product: returns a . b</td></tr>
<tr><td><code>softmax(input, output)</code></td><td>Softmax: output = softmax(input)</td></tr>
<tr><td><code>compileKernel(source)</code></td><td>Compile a kernel from portable source</td></tr>
<tr><td><code>launchKernel(kernel, config, args)</code></td><td>Launch a compiled kernel</td></tr>
<tr><td><code>synchronize()</code></td><td>Synchronize all pending operations</td></tr>
<tr><td><code>createStream(options)</code></td><td>Create a new stream</td></tr>
<tr><td><code>createEvent(options)</code></td><td>Create a new event</td></tr>
<tr><td><code>getStats()</code></td><td>Get GPU statistics</td></tr>
<tr><td><code>getMemoryInfo()</code></td><td>Get memory information</td></tr>
<tr><td><code>checkHealth()</code></td><td>Check GPU health</td></tr>
<tr><td><code>isAvailable()</code></td><td>Check if GPU is available</td></tr>
<tr><td><code>getBackend()</code></td><td>Get the active backend</td></tr>
<tr><td><code>isProfilingEnabled()</code></td><td>Check if profiling is enabled</td></tr>
<tr><td><code>enableProfiling()</code></td><td>Enable profiling</td></tr>
<tr><td><code>disableProfiling()</code></td><td>Disable profiling</td></tr>
<tr><td><code>getMetricsSummary()</code></td><td>Get metrics summary (if profiling enabled)</td></tr>
<tr><td><code>getKernelMetrics(name)</code></td><td>Get kernel-specific metrics</td></tr>
<tr><td><code>getMetricsCollector()</code></td><td>Get the metrics collector directly</td></tr>
<tr><td><code>resetMetrics()</code></td><td>Reset all profiling metrics</td></tr>
<tr><td><code>isMultiGpuEnabled()</code></td><td>Check if multi-GPU is enabled</td></tr>
<tr><td><code>getMultiGpuStats()</code></td><td>Get multi-GPU statistics (if enabled)</td></tr>
<tr><td><code>activeDeviceCount()</code></td><td>Get the number of active devices</td></tr>

<h3 id="executionresult-methods">ExecutionResult Methods<a class="anchor" href="#executionresult-methods">#</a></h3>

<tr><td>Method</td><td>Description</td></tr>
<tr><td><code>throughputGBps()</code></td><td>Get throughput in GB/s</td></tr>
<tr><td><code>elementsPerSecond()</code></td><td>Get elements per second</td></tr>

<hr>

<h2 id="see-also">See Also<a class="anchor" href="#see-also">#</a></h2>

<p>&lt;table&gt;</p>
<p>&lt;tr&gt;</p>
<p>&lt;td&gt;</p>

<h3 id="related-guides">Related Guides<a class="anchor" href="#related-guides">#</a></h3>
<li><a href="gpu-backend-improvements.md">GPU Backend Details</a> ‚Äî Implementation specifics</li>
<li><a href="compute.md">Compute Engine</a> ‚Äî CPU/GPU workload scheduling</li>
<li><a href="monitoring.md">Monitoring</a> ‚Äî GPU metrics and profiling</li>

<p>&lt;/td&gt;</p>
<p>&lt;td&gt;</p>

<h3 id="resources">Resources<a class="anchor" href="#resources">#</a></h3>
<li><a href="troubleshooting.md">Troubleshooting</a> ‚Äî GPU detection issues</li>
<li><a href="../API_REFERENCE.md">API Reference</a> ‚Äî GPU API details</li>
<li><a href="../examples/">Examples</a> ‚Äî GPU code samples</li>

<p>&lt;/td&gt;</p>
<p>&lt;/tr&gt;</p>
<p>&lt;/table&gt;</p>

<hr>

<p>&lt;p align=&quot;center&quot;&gt;</p>
<p>  &lt;a href=&quot;ai.md&quot;&gt;‚Üê AI Guide&lt;/a&gt; ‚Ä¢</p>
<p>  &lt;a href=&quot;docs-index.md&quot;&gt;Documentation Index&lt;/a&gt; ‚Ä¢</p>
<p>  &lt;a href=&quot;database.md&quot;&gt;Database Guide ‚Üí&lt;/a&gt;</p>
<p>&lt;/p&gt;</p>

</div>
    </article>
  </main>
</div>
<footer class="footer" style="margin-left: 0;">
  <div class="footer-content">
    <div class="footer-section">
      <h4>ABI Framework</h4>
      <p>Modern Zig framework for AI services and high-performance systems.</p>
    </div>
  </div>
  <div class="footer-bottom"><p>&copy; 2026 ABI Framework. Built with Zig.</p></div>
</footer>
<script src="/abi/assets/js/main.js"></script>
</body>
</html>
