const std = @import("std");
const gpu = std.gpu;

const database = @import("database.zig");
const root = @import("root.zig");

// GPU-Accelerated Backend for WDBX-AI Vector Database
//
// This module provides GPU-accelerated operations for vector similarity search,
// batch processing, and compute-intensive operations using WebGPU through std.gpu.
//
// Features:
// - GPU-accelerated vector similarity search
// - Batch vector processing
// - Compute shader-based operations
// - Asynchronous GPU operations
// - Memory-efficient GPU buffer management

/// GPU Backend configuration
pub const GpuBackendConfig = struct {
    /// Maximum number of vectors to process in a single batch
    max_batch_size: u32 = 1024,
    /// GPU memory limit in bytes
    memory_limit: u64 = 512 * 1024 * 1024, // 512MB
    /// Enable debug validation layers
    debug_validation: bool = false,
    /// Preferred GPU power preference
    power_preference: gpu.PowerPreference = .high_performance,
};

/// GPU backend context for accelerated operations
pub const GpuBackend = struct {
    /// Configuration
    config: GpuBackendConfig,
    /// Allocator for CPU-side memory
    allocator: std.mem.Allocator,
    /// GPU availability flag
    gpu_available: bool = false,
    /// Memory usage tracking
    memory_used: u64 = 0,

    /// Error types for GPU backend operations - improved based on Zig best practices
    pub const Error = error{
        /// GPU not available on this system
        GpuNotAvailable,
        /// GPU initialization failed
        GpuInitializationFailed,
        /// GPU adapter not found
        AdapterNotFound,
        /// GPU device creation failed
        DeviceCreationFailed,
        /// Compute pipeline creation failed
        PipelineCreationFailed,
        /// GPU buffer creation failed
        BufferCreationFailed,
        /// GPU operation timeout
        OperationTimeout,
        /// Insufficient GPU memory
        InsufficientGpuMemory,
        /// Shader compilation failed
        ShaderCompilationFailed,
        /// Invalid buffer size
        InvalidBufferSize,
        /// GPU operation failed
        GpuOperationFailed,
    } || std.mem.Allocator.Error;

    /// Initialize GPU backend with given configuration
    pub fn init(allocator: std.mem.Allocator, config: GpuBackendConfig) Error!*GpuBackend {
        const self = try allocator.create(GpuBackend);
        errdefer allocator.destroy(self);

        self.* = GpuBackend{
            .allocator = allocator,
            .config = config,
            .gpu_available = false,
            .memory_used = 0,
        };

        // Check if GPU is available (simplified check)
        self.gpu_available = checkGpuAvailability();

        if (!self.gpu_available) {
            // Still return a valid backend, but operations will fallback to CPU
            std.log.warn("GPU not available, using CPU fallback for compute operations", .{});
        }

        return self;
    }

    /// Clean up GPU resources
    pub fn deinit(self: *GpuBackend) void {
        self.allocator.destroy(self);
    }

    /// Check if GPU is available on the system
    fn checkGpuAvailability() bool {
        // In a real implementation, this would check for WebGPU/Vulkan/etc support
        // For now, return false to use CPU fallback
        return false;
    }

    /// GPU-accelerated vector similarity search with CPU fallback
    pub fn searchSimilar(self: *GpuBackend, db: *database.Db, query: []const f32, top_k: usize) Error![]database.Db.Result {
        // Fast path check for query validity
        if (query.len == 0) return Error.InvalidBufferSize;
        if (query.len != db.header.dim) return Error.InvalidBufferSize;

        // Use CPU fallback if GPU is not available
        if (!self.gpu_available) {
            return self.searchSimilarCpu(db, query, top_k);
        }

        // GPU implementation would go here
        return Error.GpuNotAvailable;
    }

    /// CPU fallback implementation for vector similarity search
    fn searchSimilarCpu(self: *GpuBackend, db: *database.Db, query: []const f32, top_k: usize) Error![]database.Db.Result {
        const vector_count = db.header.row_count;
        const dimension = db.header.dim;

        if (vector_count == 0) {
            return self.allocator.alloc(database.Db.Result, 0);
        }

        // Allocate results array
        var results = try self.allocator.alloc(database.Db.Result, vector_count);
        defer self.allocator.free(results);

        // Allocate buffer for reading vectors
        const vector_buffer = try self.allocator.alloc(f32, dimension);
        defer self.allocator.free(vector_buffer);

        // Calculate distances for all vectors
        const record_size = @as(u64, dimension) * @sizeOf(f32);

        for (0..vector_count) |i| {
            const offset = db.header.records_off + @as(u64, i) * record_size;
            try db.file.seekTo(offset);

            const vector_bytes = std.mem.sliceAsBytes(vector_buffer);
            try db.file.reader().readNoEof(vector_bytes);

            // Calculate squared Euclidean distance
            var distance: f32 = 0.0;
            for (vector_buffer, 0..) |val, j| {
                const diff = val - query[j];
                distance += diff * diff;
            }

            results[i] = .{
                .index = @intCast(i),
                .score = distance,
            };
        }

        // Sort results by score (ascending)
        std.mem.sort(database.Db.Result, results, {}, comptime database.Db.Result.lessThanAsc);

        // Return top-k results
        const result_count = @min(top_k, results.len);
        const final_results = try self.allocator.alloc(database.Db.Result, result_count);
        @memcpy(final_results, results[0..result_count]);

        return final_results;
    }

    /// Check if there's enough GPU memory for an operation
    pub fn hasMemoryFor(self: *const GpuBackend, bytes: u64) bool {
        return self.memory_used + bytes <= self.config.memory_limit;
    }

    /// Batch process multiple queries
    pub fn batchSearch(self: *GpuBackend, db: *database.Db, queries: []const []const f32, top_k: usize) Error![][]database.Db.Result {
        const results = try self.allocator.alloc([]database.Db.Result, queries.len);
        errdefer self.allocator.free(results);

        var completed: usize = 0;
        errdefer {
            for (results[0..completed]) |result| {
                self.allocator.free(result);
            }
        }

        for (queries, 0..) |query, i| {
            results[i] = try self.searchSimilar(db, query, top_k);
            completed += 1;
        }

        return results;
    }
};

/// Batch processing configuration
pub const BatchConfig = struct {
    /// Number of queries to process in parallel
    parallel_queries: u32 = 4,
    /// Maximum batch size
    max_batch_size: u32 = 1024,
    /// Enable progress reporting
    report_progress: bool = false,
};

/// Batch processor for multiple vector operations
pub const BatchProcessor = struct {
    backend: *GpuBackend,
    config: BatchConfig,

    pub fn init(backend: *GpuBackend, config: BatchConfig) BatchProcessor {
        return .{
            .backend = backend,
            .config = config,
        };
    }

    /// Process a batch of queries with progress reporting
    pub fn processBatch(self: *BatchProcessor, db: *database.Db, queries: []const []const f32, top_k: usize) ![][]database.Db.Result {
        const batch_count = (queries.len + self.config.max_batch_size - 1) / self.config.max_batch_size;

        if (self.config.report_progress) {
            std.log.info("Processing {} queries in {} batches", .{ queries.len, batch_count });
        }

        return self.backend.batchSearch(db, queries, top_k);
    }

    /// Process queries with a callback for each result
    pub fn processBatchWithCallback(
        self: *BatchProcessor,
        db: *database.Db,
        queries: []const []const f32,
        top_k: usize,
        callback: *const fn (index: usize, results: []database.Db.Result) void,
    ) !void {
        for (queries, 0..) |query, i| {
            const results = try self.backend.searchSimilar(db, query, top_k);
            defer self.backend.allocator.free(results);
            callback(i, results);
        }
    }
};

/// Performance statistics for GPU operations
pub const GpuStats = struct {
    /// Total operations performed
    total_operations: u64 = 0,
    /// Total time spent in GPU operations (microseconds)
    total_gpu_time: u64 = 0,
    /// Peak memory usage
    peak_memory_usage: u64 = 0,
    /// Number of fallbacks to CPU
    cpu_fallback_count: u64 = 0,

    pub fn recordOperation(self: *GpuStats, duration_us: u64, memory_used: u64, used_cpu: bool) void {
        self.total_operations += 1;
        self.total_gpu_time += duration_us;
        self.peak_memory_usage = @max(self.peak_memory_usage, memory_used);
        if (used_cpu) self.cpu_fallback_count += 1;
    }

    pub fn getAverageOperationTime(self: *const GpuStats) u64 {
        if (self.total_operations == 0) return 0;
        return self.total_gpu_time / self.total_operations;
    }

    pub fn print(self: *const GpuStats) void {
        std.log.info("GPU Backend Statistics:", .{});
        std.log.info("  Total operations: {}", .{self.total_operations});
        std.log.info("  Average operation time: {}Î¼s", .{self.getAverageOperationTime()});
        std.log.info("  Peak memory usage: {} MB", .{self.peak_memory_usage / (1024 * 1024)});
        std.log.info("  CPU fallback rate: {d:.2}%", .{if (self.total_operations > 0)
            @as(f64, @floatFromInt(self.cpu_fallback_count)) * 100.0 / @as(f64, @floatFromInt(self.total_operations))
        else
            0.0});
    }
};
