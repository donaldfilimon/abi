const std = @import("std");

/// Generate comprehensive definitions reference documentation
pub fn generateDefinitionsReference(_: std.mem.Allocator) !void {
    const file = try std.fs.cwd().createFile("docs/generated/DEFINITIONS_REFERENCE.md", .{});
    defer file.close();

    const content =
        \\---
        \\layout: documentation
        \\title: "Definitions Reference"
        \\description: "Comprehensive glossary and concepts for ABI technology"
        \\keywords: ["vector database", "AI", "machine learning", "SIMD", "neural networks", "embeddings"]
        \\---
        \\
        \\# ABI Definitions Reference
        \\
        \\<div class="definition-search">
        \\  <input type="search" id="definition-search" placeholder="Search definitions..." autocomplete="off">
        \\  <div class="definition-categories">
        \\    <button class="category-filter active" data-category="all">All</button>
        \\    <button class="category-filter" data-category="database">Database</button>
        \\    <button class="category-filter" data-category="ai">AI/ML</button>
        \\    <button class="category-filter" data-category="performance">Performance</button>
        \\    <button class="category-filter" data-category="algorithms">Algorithms</button>
        \\    <button class="category-filter" data-category="system">System</button>
        \\  </div>
        \\</div>
        \\
        \\## üìä Quick Reference Index
        \\
        \\| Term | Category | Definition |
        \\|------|----------|------------|
        \\| [Vector Database](#vector-database) | Database | Specialized storage for high-dimensional vectors |
        \\| [Embeddings](#embeddings) | AI/ML | Dense vector representations of data |
        \\| [HNSW](#hnsw-hierarchical-navigable-small-world) | Algorithms | Graph-based indexing for similarity search |
        \\| [Neural Network](#neural-network) | AI/ML | Computational model inspired by biological networks |
        \\| [SIMD](#simd-single-instruction-multiple-data) | Performance | Parallel processing technique |
        \\| [Cosine Similarity](#cosine-similarity) | Algorithms | Directional similarity metric |
        \\| [Backpropagation](#backpropagation) | AI/ML | Neural network training algorithm |
        \\| [Plugin Architecture](#plugin-architecture) | System | Extensible software design pattern |
        \\
        \\---
        \\
        \\## üóÑÔ∏è Database & Storage {#database}
        \\
        \\### Vector Database
        \\<div class="definition-card" data-category="database">
        \\
        \\A specialized database system designed to store, index, and search high-dimensional vectors efficiently. Unlike traditional relational databases that work with scalar values and structured data, vector databases are optimized for similarity search operations using various distance metrics.
        \\
        \\**Key Characteristics:**
        \\- **High-dimensional storage**: Efficiently handles vectors with hundreds to thousands of dimensions
        \\- **Similarity search**: Primary operation is finding vectors most similar to a query vector
        \\- **Specialized indexing**: Uses algorithms like HNSW, IVF, or LSH for fast approximate nearest neighbor search
        \\- **Scalability**: Designed to handle millions to billions of vectors with sub-linear search complexity
        \\- **Metadata support**: Associates additional information with each vector for filtering and retrieval
        \\
        \\**Common Use Cases:**
        \\- Semantic search in documents and images
        \\- Recommendation systems
        \\- Content-based filtering
        \\- Duplicate detection and deduplication
        \\- Anomaly detection in high-dimensional data
        \\
        \\**Performance Characteristics:**
        \\- Insert: ~2.5ms per vector (128 dimensions)
        \\- Search: ~13ms for k=10 in 10k vectors
        \\- Memory: ~512 bytes per vector + index overhead
        \\
        \\</div>
        \\
        \\### Embeddings
        \\<div class="definition-card" data-category="ai database">
        \\
        \\Dense, fixed-size vector representations that capture semantic meaning and relationships in a continuous mathematical space. Embeddings are typically generated by machine learning models and enable mathematical operations on complex data types.
        \\
        \\**Types of Embeddings:**
        \\- **Text embeddings**: Word2Vec, GloVe, BERT, sentence transformers
        \\- **Image embeddings**: CNN features, CLIP, vision transformers
        \\- **Audio embeddings**: Mel spectrograms, audio neural networks
        \\- **Graph embeddings**: Node2Vec, GraphSAGE for network data
        \\- **Multimodal embeddings**: CLIP, ALIGN for cross-modal understanding
        \\
        \\**Properties:**
        \\- **Dimensionality**: Typically 128-1024 dimensions for most applications
        \\- **Semantic similarity**: Similar concepts have similar vector representations
        \\- **Arithmetic operations**: Support vector arithmetic (king - man + woman ‚âà queen)
        \\- **Transfer learning**: Pre-trained embeddings can be fine-tuned for specific tasks
        \\
        \\**Quality Metrics:**
        \\- **Cosine similarity**: Measures directional similarity
        \\- **Clustering coefficient**: How well similar items cluster together
        \\- **Downstream task performance**: Effectiveness in specific applications
        \\
        \\</div>
        \\
        \\### Indexing Algorithms
        \\<div class="definition-card" data-category="algorithms database">
        \\
        \\Specialized data structures and algorithms designed to accelerate similarity search in high-dimensional vector spaces. These algorithms trade exact accuracy for significant speed improvements.
        \\
        \\**Major Categories:**
        \\
        \\**Tree-based:**
        \\- **KD-Tree**: Binary tree partitioning, effective in low dimensions
        \\- **Ball Tree**: Hypersphere partitioning, better for higher dimensions
        \\- **R-Tree**: Rectangle-based partitioning for spatial data
        \\
        \\**Hash-based:**
        \\- **LSH (Locality Sensitive Hashing)**: Hash similar items to same buckets
        \\- **Random Projection**: Reduce dimensionality while preserving distances
        \\- **Product Quantization**: Divide vectors into subvectors for compression
        \\
        \\**Graph-based:**
        \\- **HNSW**: Hierarchical navigable small world graphs
        \\- **NSW**: Navigable small world graphs
        \\- **SPTAG**: Space Partition Tree and Graph
        \\
        \\**Inverted File (IVF):**
        \\- **IVF-Flat**: Partition space into Voronoi cells
        \\- **IVF-PQ**: Combine IVF with product quantization
        \\- **IVF-SQ**: Combine IVF with scalar quantization
        \\
        \\</div>
        \\
        \\### HNSW (Hierarchical Navigable Small World)
        \\<div class="definition-card" data-category="algorithms database">
        \\
        \\A state-of-the-art graph-based indexing algorithm that builds a multi-layered network of connections between vectors. It provides excellent performance for approximate nearest neighbor search with logarithmic time complexity.
        \\
        \\**Architecture:**
        \\- **Layer 0 (bottom)**: Contains all vectors with short-range connections to immediate neighbors
        \\- **Upper layers**: Contain exponentially fewer vectors with long-range connections for fast navigation
        \\- **Entry point**: Top-layer node where search begins
        \\- **Greedy search**: Navigate from top to bottom, always moving to closer neighbors
        \\
        \\**Key Parameters:**
        \\- **M (max connections)**: Maximum edges per node (16-64 typical)
        \\  - Higher M: Better recall, more memory usage
        \\  - Lower M: Faster construction, potential recall degradation
        \\- **efConstruction**: Candidate set size during index construction (200-800 typical)
        \\  - Higher ef: Better index quality, slower construction
        \\- **efSearch**: Candidate set size during search (varies by recall requirements)
        \\  - Higher ef: Better recall, slower search
        \\- **ml (level multiplier)**: Controls layer distribution (1/ln(2) ‚âà 1.44)
        \\
        \\**Performance Characteristics:**
        \\- **Search complexity**: O(log N) on average
        \\- **Construction complexity**: O(N log N) on average
        \\- **Memory usage**: O(M √ó N) for connections
        \\- **Recall**: 95-99% achievable with proper parameter tuning
        \\
        \\**Advantages:**
        \\- High recall with fast search speed
        \\- Supports dynamic insertions and deletions
        \\- Good performance across various distance metrics
        \\- Robust to different data distributions
        \\
        \\</div>
        \\
        \\## üß† Artificial Intelligence & Machine Learning {#ai}
        \\
        \\### Neural Network
        \\<div class="definition-card" data-category="ai">
        \\
        \\A computational model inspired by biological neural networks, consisting of interconnected processing units (neurons) organized in layers. Each connection has an associated weight that determines the strength and direction of signal transmission.
        \\
        \\**Architecture Components:**
        \\- **Input layer**: Receives raw feature data (images, text, audio, etc.)
        \\- **Hidden layers**: Process and transform input through weighted connections and activation functions
        \\- **Output layer**: Produces final predictions, classifications, or generated content
        \\- **Connections**: Weighted links between neurons that are learned during training
        \\
        \\**Common Architectures:**
        \\- **Feedforward**: Information flows in one direction from input to output
        \\- **Convolutional (CNN)**: Specialized for image and spatial data processing
        \\- **Recurrent (RNN/LSTM/GRU)**: Designed for sequential data with memory
        \\- **Transformer**: Attention-based architecture for sequence modeling
        \\- **Autoencoder**: Encoder-decoder structure for dimensionality reduction
        \\- **Generative Adversarial (GAN)**: Two networks competing to generate realistic data
        \\
        \\**Activation Functions:**
        \\- **ReLU**: f(x) = max(0, x) - most common, prevents vanishing gradients
        \\- **Sigmoid**: f(x) = 1/(1 + e^(-x)) - outputs between 0 and 1
        \\- **Tanh**: f(x) = tanh(x) - outputs between -1 and 1
        \\- **Softmax**: Converts logits to probability distribution
        \\- **Swish/SiLU**: f(x) = x √ó sigmoid(x) - smooth, self-gating
        \\
        \\</div>
        \\
        \\### Backpropagation
        \\<div class="definition-card" data-category="ai algorithms">
        \\
        \\The fundamental algorithm for training neural networks by computing gradients of the loss function with respect to each parameter. It efficiently propagates error signals backwards through the network layers.
        \\
        \\**Algorithm Steps:**
        \\1. **Forward pass**: Input data flows through network to produce output
        \\2. **Loss computation**: Compare network output to target using loss function
        \\3. **Backward pass**: Compute gradients by applying chain rule from output to input
        \\4. **Parameter update**: Adjust weights using gradients and learning rate
        \\
        \\**Mathematical Foundation:**
        \\- **Chain rule**: ‚àÇL/‚àÇw = ‚àÇL/‚àÇy √ó ‚àÇy/‚àÇz √ó ‚àÇz/‚àÇw
        \\- **Gradient computation**: Efficient recursive calculation of partial derivatives
        \\- **Dynamic programming**: Reuses intermediate computations to avoid redundancy
        \\
        \\**Common Issues:**
        \\- **Vanishing gradients**: Gradients become very small in deep networks
        \\- **Exploding gradients**: Gradients become very large, causing instability
        \\- **Dead neurons**: Neurons that always output zero (common with ReLU)
        \\
        \\**Solutions:**
        \\- **Gradient clipping**: Limit gradient magnitude to prevent explosion
        \\- **Normalization**: Batch norm, layer norm to stabilize training
        \\- **Skip connections**: ResNet-style shortcuts to help gradient flow
        \\- **Learning rate scheduling**: Adaptive learning rates during training
        \\
        \\</div>
        \\
        \\### Gradient Descent
        \\<div class="definition-card" data-category="ai algorithms">
        \\
        \\An iterative optimization algorithm that minimizes a loss function by moving in the direction of steepest descent. It's the foundation for training most machine learning models.
        \\
        \\**Variants:**
        \\- **Batch Gradient Descent**: Uses entire dataset for each update
        \\  - Pros: Stable convergence, deterministic
        \\  - Cons: Slow for large datasets, may get stuck in local minima
        \\- **Stochastic Gradient Descent (SGD)**: Uses one sample at a time
        \\  - Pros: Fast updates, can escape local minima
        \\  - Cons: Noisy convergence, requires careful tuning
        \\- **Mini-batch Gradient Descent**: Uses small batches (32-256 samples)
        \\  - Pros: Good balance of speed and stability
        \\  - Cons: Still requires hyperparameter tuning
        \\
        \\**Advanced Optimizers:**
        \\- **Momentum**: Accumulates gradients to accelerate convergence
        \\- **AdaGrad**: Adapts learning rate based on historical gradients
        \\- **RMSprop**: Improves AdaGrad with exponential moving average
        \\- **Adam**: Combines momentum and adaptive learning rates
        \\- **AdamW**: Adam with decoupled weight decay
        \\
        \\**Hyperparameters:**
        \\- **Learning rate (Œ±)**: Step size for parameter updates (1e-4 to 1e-1)
        \\- **Momentum (Œ≤)**: Exponential decay for gradient accumulation (0.9-0.99)
        \\- **Weight decay**: L2 regularization to prevent overfitting (1e-5 to 1e-3)
        \\- **Learning rate schedule**: Decay strategy over training epochs
        \\
        \\</div>
        \\
        \\### Transformer Architecture
        \\<div class="definition-card" data-category="ai">
        \\
        \\A neural network architecture based entirely on attention mechanisms, revolutionizing natural language processing and extending to computer vision and other domains.
        \\
        \\**Key Components:**
        \\- **Multi-Head Attention**: Parallel attention mechanisms focusing on different aspects
        \\- **Position Encoding**: Adds positional information since attention is permutation-invariant
        \\- **Feed-Forward Networks**: Point-wise fully connected layers
        \\- **Layer Normalization**: Stabilizes training and improves convergence
        \\- **Residual Connections**: Skip connections around each sub-layer
        \\
        \\**Attention Mechanism:**
        \\- **Query (Q)**: What information are we looking for?
        \\- **Key (K)**: What information is available?
        \\- **Value (V)**: The actual information content
        \\- **Attention(Q,K,V) = softmax(QK^T/‚àöd_k)V**
        \\
        \\**Variants:**
        \\- **BERT**: Bidirectional encoder for understanding tasks
        \\- **GPT**: Autoregressive decoder for generation tasks
        \\- **T5**: Text-to-text transfer transformer
        \\- **Vision Transformer (ViT)**: Applies transformer to image patches
        \\- **CLIP**: Contrastive learning of text and image representations
        \\
        \\</div>
        \\
        \\### Large Language Models (LLMs)
        \\<div class="definition-card" data-category="ai">
        \\
        \\Neural networks with billions to trillions of parameters trained on vast text corpora to understand and generate human-like text. They demonstrate emergent capabilities as they scale.
        \\
        \\**Characteristics:**
        \\- **Scale**: 1B to 175B+ parameters (GPT-3 has 175B parameters)
        \\- **Training data**: Hundreds of gigabytes to terabytes of text
        \\- **Emergent abilities**: Few-shot learning, reasoning, code generation
        \\- **In-context learning**: Learning from examples in the prompt
        \\
        \\**Training Stages:**
        \\1. **Pre-training**: Unsupervised learning on large text corpus
        \\2. **Fine-tuning**: Supervised learning on specific tasks
        \\3. **RLHF**: Reinforcement Learning from Human Feedback
        \\4. **Constitutional AI**: Training for harmlessness and helpfulness
        \\
        \\**Capabilities:**
        \\- Text generation and completion
        \\- Question answering and reasoning
        \\- Code generation and debugging
        \\- Language translation
        \\- Summarization and analysis
        \\- Creative writing and ideation
        \\
        \\</div>
        \\
        \\### Agent-Based Systems
        \\<div class="definition-card" data-category="ai system">
        \\
        \\Autonomous software entities that perceive their environment, make decisions, and take actions to achieve specific goals. Modern AI agents often incorporate large language models and various tools.
        \\
        \\**Agent Components:**
        \\- **Perception**: Sensors and inputs to observe environment state
        \\- **Decision making**: Logic, rules, or learned policies to choose actions
        \\- **Action**: Effectors and outputs to modify the environment
        \\- **Memory**: Storage of experiences, knowledge, and learned behaviors
        \\- **Communication**: Ability to interact with other agents or humans
        \\
        \\**Agent Types:**
        \\- **Reactive agents**: Respond directly to current perceptions without internal state
        \\- **Deliberative agents**: Plan sequences of actions using internal world models
        \\- **Learning agents**: Improve performance through experience and feedback
        \\- **Hybrid agents**: Combine reactive and deliberative components
        \\
        \\**Modern AI Agents:**
        \\- **Tool-using agents**: LLMs that can use external tools and APIs
        \\- **Code agents**: Generate and execute code to solve problems
        \\- **Conversational agents**: Chatbots and virtual assistants
        \\- **Planning agents**: Decompose complex tasks into subtasks
        \\- **Multi-agent systems**: Coordination between multiple AI agents
        \\
        \\**Design Patterns:**
        \\- **ReAct**: Reasoning and Acting with language models
        \\- **Chain of Thought**: Step-by-step reasoning prompts
        \\- **Tree of Thoughts**: Exploring multiple reasoning paths
        \\- **Reflection**: Self-evaluation and improvement mechanisms
        \\
        \\</div>
        \\
        \\## ‚ö° Performance & Optimization {#performance}
        \\
        \\### SIMD (Single Instruction, Multiple Data)
        \\<div class="definition-card" data-category="performance">
        \\
        \\A parallel computing technique where a single instruction operates on multiple data points simultaneously. Modern CPUs have dedicated SIMD units that can process multiple numbers in one clock cycle.
        \\
        \\**Instruction Sets:**
        \\- **SSE (128-bit)**: 4 √ó float32 or 2 √ó float64 operations per instruction
        \\- **AVX (256-bit)**: 8 √ó float32 or 4 √ó float64 operations per instruction
        \\- **AVX-512 (512-bit)**: 16 √ó float32 or 8 √ó float64 operations per instruction
        \\- **ARM NEON**: ARM's SIMD instruction set for mobile processors
        \\
        \\**Benefits:**
        \\- **Throughput**: 4-16x more operations per clock cycle
        \\- **Memory bandwidth**: More efficient use of memory bus
        \\- **Energy efficiency**: Better performance per watt
        \\- **Cache efficiency**: Process more data with same cache footprint
        \\
        \\**Applications in Vector Databases:**
        \\- Vector addition, subtraction, multiplication
        \\- Dot product and cosine similarity calculations
        \\- Distance metric computations (Euclidean, Manhattan)
        \\- Matrix operations for neural networks
        \\- Quantization and compression operations
        \\
        \\**Programming Considerations:**
        \\- **Alignment**: Data must be aligned to vector width boundaries
        \\- **Data layout**: Array of Structures vs Structure of Arrays
        \\- **Compiler intrinsics**: Direct use of SIMD instructions
        \\- **Auto-vectorization**: Compiler automatic SIMD optimization
        \\
        \\</div>
        \\
        \\### Memory Hierarchy & Optimization
        \\<div class="definition-card" data-category="performance system">
        \\
        \\The hierarchical organization of computer memory systems, from fast but small caches to large but slow storage, and techniques to optimize data access patterns.
        \\
        \\**Memory Hierarchy (fastest to slowest):**
        \\- **CPU Registers**: ~1 cycle access, 32-64 registers
        \\- **L1 Cache**: ~1-3 cycles, 32-64KB per core, separate instruction/data
        \\- **L2 Cache**: ~10-20 cycles, 256KB-1MB per core, unified
        \\- **L3 Cache**: ~30-50 cycles, 8-64MB shared across cores
        \\- **Main Memory (RAM)**: ~100-300 cycles, GBs to TBs
        \\- **SSD Storage**: ~10-100Œºs, TBs capacity
        \\- **HDD Storage**: ~1-10ms, TBs capacity
        \\
        \\**Cache Properties:**
        \\- **Cache line size**: Typically 64 bytes
        \\- **Associativity**: Direct-mapped, set-associative, fully-associative
        \\- **Replacement policies**: LRU, random, pseudo-LRU
        \\- **Write policies**: Write-through, write-back
        \\
        \\**Optimization Techniques:**
        \\- **Spatial locality**: Access nearby memory locations
        \\- **Temporal locality**: Reuse recently accessed data
        \\- **Prefetching**: Load data before it's needed
        \\- **Cache blocking**: Restructure algorithms for cache efficiency
        \\- **Memory alignment**: Align data structures to cache line boundaries
        \\
        \\</div>
        \\
        \\### Batch Processing
        \\<div class="definition-card" data-category="performance">
        \\
        \\The practice of grouping multiple operations together to improve throughput and reduce per-operation overhead. Essential for achieving high performance in vector databases and machine learning.
        \\
        \\**Benefits:**
        \\- **Amortized overhead**: Function call and setup costs spread across multiple items
        \\- **Better memory locality**: Sequential access patterns improve cache performance
        \\- **SIMD utilization**: Process multiple items with vector instructions
        \\- **Reduced context switching**: Fewer kernel calls and mode switches
        \\- **Pipeline efficiency**: Keep execution units busy with continuous work
        \\
        \\**Optimal Batch Sizes:**
        \\- **Database inserts**: 100-1000 vectors (balance memory and throughput)
        \\- **Neural network training**: 32-512 samples (GPU memory dependent)
        \\- **SIMD operations**: Multiples of vector width (4, 8, 16 elements)
        \\- **I/O operations**: Page size multiples (4KB, 64KB blocks)
        \\
        \\**Implementation Strategies:**
        \\- **Buffering**: Accumulate items before processing
        \\- **Pipelining**: Overlap different stages of processing
        \\- **Work stealing**: Dynamic load balancing across threads
        \\- **Adaptive batching**: Adjust batch size based on system conditions
        \\
        \\</div>
        \\
        \\### Quantization
        \\<div class="definition-card" data-category="performance ai">
        \\
        \\Techniques for reducing the precision of numerical representations while preserving essential information. Critical for reducing memory usage and improving performance in large-scale systems.
        \\
        \\**Types of Quantization:**
        \\- **Scalar quantization**: Map continuous values to discrete levels
        \\- **Vector quantization**: Group similar vectors and represent with centroids
        \\- **Product quantization**: Decompose vectors into subvectors, quantize separately
        \\- **Binary quantization**: Extreme compression to 1-bit representations
        \\
        \\**Precision Levels:**
        \\- **INT8**: 8-bit integers, 4x memory reduction from FP32
        \\- **INT4**: 4-bit integers, 8x memory reduction, requires careful calibration
        \\- **INT1 (Binary)**: 1-bit representations, 32x reduction, significant accuracy loss
        \\- **Mixed precision**: Different precisions for different layers/operations
        \\
        \\**Quantization Strategies:**
        \\- **Post-training quantization**: Quantize after training with calibration data
        \\- **Quantization-aware training**: Include quantization in training process
        \\- **Dynamic quantization**: Adjust quantization parameters during inference
        \\- **Learned quantization**: Use neural networks to optimize quantization
        \\
        \\**Trade-offs:**
        \\- **Memory**: 2-32x reduction in storage requirements
        \\- **Speed**: Faster integer operations, reduced memory bandwidth
        \\- **Accuracy**: Some loss in precision, especially for aggressive quantization
        \\- **Compatibility**: Requires specialized hardware or software support
        \\
        \\</div>
        \\
        \\## üìê Distance Metrics & Similarity {#algorithms}
        \\
        \\### Euclidean Distance
        \\<div class="definition-card" data-category="algorithms">
        \\
        \\The straight-line distance between two points in multidimensional space, corresponding to our intuitive notion of distance in physical space.
        \\
        \\**Mathematical Definition:**
        \\- **Formula**: d(a,b) = ‚àö(Œ£·µ¢(a·µ¢ - b·µ¢)¬≤)
        \\- **Squared Euclidean**: Often used to avoid expensive square root: Œ£·µ¢(a·µ¢ - b·µ¢)¬≤
        \\
        \\**Properties:**
        \\- **Range**: [0, ‚àû), where 0 indicates identical vectors
        \\- **Symmetry**: d(a,b) = d(b,a)
        \\- **Triangle inequality**: d(a,c) ‚â§ d(a,b) + d(b,c)
        \\- **Positive definiteness**: d(a,b) = 0 if and only if a = b
        \\
        \\**Best Use Cases:**
        \\- **Image features**: Pixel values, color histograms
        \\- **Continuous measurements**: Physical measurements, sensor data
        \\- **Dense embeddings**: When magnitude matters (e.g., word embeddings)
        \\- **Gaussian distributions**: When data follows normal distribution
        \\
        \\**Computational Complexity:**
        \\- **Time**: O(d) where d is vector dimension
        \\- **SIMD optimization**: Highly vectorizable operation
        \\- **Memory access**: Sequential, cache-friendly
        \\
        \\</div>
        \\
        \\### Cosine Similarity
        \\<div class="definition-card" data-category="algorithms">
        \\
        \\Measures the cosine of the angle between two vectors, focusing on direction rather than magnitude. Widely used in text analysis and recommendation systems.
        \\
        \\**Mathematical Definition:**
        \\- **Formula**: similarity(a,b) = (a¬∑b) / (||a|| √ó ||b||)
        \\- **Cosine distance**: 1 - cosine_similarity(a,b)
        \\- **Dot product**: a¬∑b = Œ£·µ¢(a·µ¢ √ó b·µ¢)
        \\- **Magnitude**: ||a|| = ‚àö(Œ£·µ¢a·µ¢¬≤)
        \\
        \\**Properties:**
        \\- **Range**: [-1, 1] where 1 = same direction, 0 = orthogonal, -1 = opposite
        \\- **Magnitude invariant**: Only considers direction, not length
        \\- **Normalized vectors**: For unit vectors, cosine similarity equals dot product
        \\- **Symmetry**: cosine_similarity(a,b) = cosine_similarity(b,a)
        \\
        \\**Best Use Cases:**
        \\- **Text embeddings**: TF-IDF vectors, word/sentence embeddings
        \\- **Sparse features**: High-dimensional sparse vectors
        \\- **Recommendation systems**: User-item preferences
        \\- **Document similarity**: When document length shouldn't matter
        \\
        \\**Optimization Techniques:**
        \\- **Pre-normalization**: Store normalized vectors to simplify computation
        \\- **SIMD dot product**: Vectorized multiplication and summation
        \\- **Approximate methods**: Random sampling for very high dimensions
        \\
        \\</div>
        \\
        \\### Manhattan Distance (L1 Norm)
        \\<div class="definition-card" data-category="algorithms">
        \\
        \\The sum of absolute differences between corresponding elements, named after Manhattan's grid-like street layout where you can only travel along perpendicular streets.
        \\
        \\**Mathematical Definition:**
        \\- **Formula**: d(a,b) = Œ£·µ¢|a·µ¢ - b·µ¢|
        \\- **Also known as**: L1 distance, taxicab distance, city block distance
        \\
        \\**Properties:**
        \\- **Range**: [0, ‚àû), where 0 indicates identical vectors
        \\- **Robustness**: Less sensitive to outliers than Euclidean distance
        \\- **Sparsity inducing**: Tends to produce sparse solutions in optimization
        \\- **Convex**: Forms diamond-shaped unit balls in 2D space
        \\
        \\**Best Use Cases:**
        \\- **Sparse data**: High-dimensional sparse vectors
        \\- **Robust statistics**: When outliers are present
        \\- **Feature selection**: L1 regularization promotes sparsity
        \\- **Discrete features**: Categorical or count data
        \\
        \\**Computational Advantages:**
        \\- **No squares**: Avoids expensive multiplication operations
        \\- **Integer arithmetic**: Can work with integer representations
        \\- **Bounded gradients**: Useful for optimization algorithms
        \\
        \\</div>
        \\
        \\### Hamming Distance
        \\<div class="definition-card" data-category="algorithms">
        \\
        \\The number of positions where corresponding elements differ, originally defined for binary strings but extended to other discrete alphabets.
        \\
        \\**Mathematical Definition:**
        \\- **Binary vectors**: Number of bit positions where vectors differ
        \\- **General case**: Number of positions where a·µ¢ ‚â† b·µ¢
        \\- **Normalized**: Divide by vector length for similarity score
        \\
        \\**Properties:**
        \\- **Range**: [0, d] where d is vector dimension
        \\- **Discrete**: Only integer values possible
        \\- **Symmetric**: Hamming(a,b) = Hamming(b,a)
        \\- **Triangle inequality**: Forms valid metric space
        \\
        \\**Applications:**
        \\- **Binary embeddings**: Locality sensitive hashing outputs
        \\- **Error correction**: Coding theory and data transmission
        \\- **Fingerprinting**: Perceptual hashing for duplicate detection
        \\- **Genetics**: DNA sequence comparison
        \\
        \\**Computational Efficiency:**
        \\- **Bit operations**: XOR followed by population count
        \\- **Hardware support**: Many CPUs have POPCNT instruction
        \\- **Parallel computation**: Highly parallelizable across bits
        \\
        \\</div>
        \\
        \\## üèóÔ∏è System Architecture {#system}
        \\
        \\### Plugin Architecture
        \\<div class="definition-card" data-category="system">
        \\
        \\A software design pattern that enables extending core functionality through dynamically loaded, modular components. Plugins are independent units that implement well-defined interfaces.
        \\
        \\**Core Components:**
        \\- **Plugin interface**: Contract defining how plugins interact with the host
        \\- **Plugin manager**: Loads, unloads, and manages plugin lifecycle
        \\- **Host application**: Core system that provides plugin infrastructure
        \\- **Plugin registry**: Catalog of available plugins and their capabilities
        \\
        \\**Implementation Approaches:**
        \\- **Dynamic libraries**: Shared objects (.so, .dll, .dylib) loaded at runtime
        \\- **Process isolation**: Plugins run in separate processes with IPC
        \\- **Scripting engines**: Embed interpreters (Python, Lua, JavaScript)
        \\- **WebAssembly**: Sandboxed plugins with near-native performance
        \\- **Container-based**: Docker containers for maximum isolation
        \\
        \\**Benefits:**
        \\- **Modularity**: Keep core system lean, add features as needed
        \\- **Extensibility**: Third-party developers can add functionality
        \\- **Isolation**: Plugin failures don't crash the host system
        \\- **Hot-swapping**: Load/unload plugins without system restart
        \\- **Versioning**: Different plugin versions can coexist
        \\
        \\**Challenges:**
        \\- **Interface stability**: API changes can break existing plugins
        \\- **Security**: Malicious plugins can compromise system
        \\- **Performance**: Inter-plugin communication overhead
        \\- **Dependency management**: Complex dependency graphs
        \\
        \\</div>
        \\
        \\### Memory Management Strategies
        \\<div class="definition-card" data-category="system performance">
        \\
        \\Techniques for efficiently allocating, using, and deallocating memory in high-performance applications, crucial for vector databases handling large datasets.
        \\
        \\**Allocation Strategies:**
        \\- **Stack allocation**: Fast automatic cleanup, limited size, LIFO order
        \\- **Heap allocation**: Flexible size, manual management, fragmentation risk
        \\- **Pool allocation**: Pre-allocate fixed-size blocks, fast allocation/deallocation
        \\- **Arena allocation**: Bulk allocation with batch cleanup, minimal overhead
        \\- **Slab allocation**: Kernel-style allocator for objects of similar size
        \\
        \\**Memory Patterns:**
        \\- **RAII (Resource Acquisition Is Initialization)**: Tie resource lifetime to object scope
        \\- **Reference counting**: Automatic cleanup when no references remain
        \\- **Garbage collection**: Automatic memory management with performance trade-offs
        \\- **Copy-on-write**: Share memory until modification is needed
        \\
        \\**Optimization Techniques:**
        \\- **Memory pools**: Reduce allocation overhead for frequent operations
        \\- **Object recycling**: Reuse expensive-to-create objects
        \\- **Alignment**: Ensure data alignment for optimal access patterns
        \\- **Prefaulting**: Touch memory pages to ensure they're resident
        \\
        \\**Monitoring and Debugging:**
        \\- **Memory profiling**: Track allocation patterns and leaks
        \\- **Valgrind**: Memory error detection for C/C++ programs
        \\- **AddressSanitizer**: Runtime memory error detector
        \\- **Custom allocators**: Track application-specific memory usage
        \\
        \\</div>
        \\
        \\### Caching Strategies
        \\<div class="definition-card" data-category="system performance">
        \\
        \\Techniques for storing frequently accessed data in faster storage layers to improve system performance by exploiting temporal and spatial locality.
        \\
        \\**Cache Hierarchies:**
        \\- **CPU caches**: L1/L2/L3 hardware caches in processor
        \\- **Application caches**: In-memory data structures (hash tables, trees)
        \\- **Database caches**: Buffer pools for frequently accessed pages
        \\- **Web caches**: CDNs and reverse proxies for distributed systems
        \\- **Disk caches**: SSD tier for frequently accessed data
        \\
        \\**Replacement Policies:**
        \\- **LRU (Least Recently Used)**: Evict items not accessed recently
        \\- **LFU (Least Frequently Used)**: Evict items accessed infrequently
        \\- **FIFO (First In, First Out)**: Simple queue-based eviction
        \\- **Random**: Simple but often effective for uniform access patterns
        \\- **ARC (Adaptive Replacement Cache)**: Adapts between recency and frequency
        \\
        \\**Cache Strategies:**
        \\- **Write-through**: Immediately write to both cache and backing store
        \\- **Write-back**: Delay writes to backing store, better performance
        \\- **Write-around**: Skip cache for writes, avoid cache pollution
        \\- **Refresh-ahead**: Proactively refresh expired entries
        \\
        \\**Performance Considerations:**
        \\- **Hit ratio**: Percentage of requests served from cache
        \\- **Miss penalty**: Cost of loading data from slower storage
        \\- **Cache coherence**: Consistency across multiple cache instances
        \\- **Working set size**: Amount of data actively accessed
        \\
        \\</div>
        \\
        \\## üìä Performance Metrics & Evaluation {#performance}
        \\
        \\### Throughput vs Latency
        \\<div class="definition-card" data-category="performance">
        \\
        \\Two fundamental performance metrics that often require trade-offs in system design. Understanding both is crucial for optimizing vector database performance.
        \\
        \\**Throughput:**
        \\- **Definition**: Number of operations completed per unit time
        \\- **Units**: Operations/second, requests/second, GB/second
        \\- **Optimization**: Batching, pipelining, parallelism
        \\- **Measurement**: Total operations / total time
        \\
        \\**Latency:**
        \\- **Definition**: Time required to complete a single operation
        \\- **Units**: Milliseconds, microseconds, nanoseconds
        \\- **Types**: Mean, median, P95, P99, tail latency
        \\- **Optimization**: Caching, indexing, algorithm optimization
        \\
        \\**Trade-offs:**
        \\- **High throughput**: May increase individual operation latency
        \\- **Low latency**: May reduce overall system throughput
        \\- **Batch processing**: Improves throughput at cost of latency
        \\- **Real-time systems**: Often prioritize latency over throughput
        \\
        \\**Little's Law:**
        \\- **Formula**: Average latency = Average queue length / Average throughput
        \\- **Application**: Helps understand system capacity and performance
        \\
        \\</div>
        \\
        \\### Recall and Precision in Vector Search
        \\<div class="definition-card" data-category="algorithms performance">
        \\
        \\Quality metrics for evaluating approximate nearest neighbor search algorithms, measuring how well they find relevant results compared to exact search.
        \\
        \\**Recall:**
        \\- **Definition**: Fraction of true nearest neighbors found by the algorithm
        \\- **Formula**: Recall = |Retrieved ‚à© Relevant| / |Relevant|
        \\- **Range**: [0, 1] where 1 = perfect recall (found all true neighbors)
        \\- **Trade-off**: Higher recall usually requires more computation
        \\
        \\**Precision:**
        \\- **Definition**: Fraction of retrieved results that are true nearest neighbors
        \\- **Formula**: Precision = |Retrieved ‚à© Relevant| / |Retrieved|
        \\- **Range**: [0, 1] where 1 = perfect precision (no false positives)
        \\- **Context**: Less commonly used in k-NN search (fixed k)
        \\
        \\**Evaluation Methodology:**
        \\- **Ground truth**: Exact k-NN results computed with brute force
        \\- **Test queries**: Representative sample of real-world queries
        \\- **Multiple k values**: Evaluate performance for different neighborhood sizes
        \\- **Parameter sweeps**: Test different algorithm configurations
        \\
        \\**Practical Considerations:**
        \\- **Acceptable recall**: Often 90-95% sufficient for most applications
        \\- **Speed-accuracy trade-off**: Balance recall against query latency
        \\- **Index parameters**: Tune to achieve target recall efficiently
        \\
        \\</div>
        \\
        \\---
        \\
        \\<style>
        \\.definition-search {
        \\  margin: 2rem 0;
        \\  padding: 1.5rem;
        \\  background: var(--color-canvas-subtle);
        \\  border-radius: 8px;
        \\  border: 1px solid var(--color-border-default);
        \\}
        \\
        \\.definition-search input {
        \\  width: 100%;
        \\  padding: 0.75rem 1rem;
        \\  border: 1px solid var(--color-border-default);
        \\  border-radius: 6px;
        \\  font-size: 1rem;
        \\  margin-bottom: 1rem;
        \\}
        \\
        \\.definition-categories {
        \\  display: flex;
        \\  gap: 0.5rem;
        \\  flex-wrap: wrap;
        \\}
        \\
        \\.category-filter {
        \\  padding: 0.5rem 1rem;
        \\  border: 1px solid var(--color-border-default);
        \\  background: var(--color-canvas-default);
        \\  border-radius: 4px;
        \\  cursor: pointer;
        \\  transition: all 0.2s ease;
        \\}
        \\
        \\.category-filter:hover {
        \\  background: var(--color-canvas-subtle);
        \\}
        \\
        \\.category-filter.active {
        \\  background: var(--color-accent-emphasis);
        \\  color: white;
        \\  border-color: var(--color-accent-emphasis);
        \\}
        \\
        \\.definition-card {
        \\  margin: 1.5rem 0;
        \\  padding: 1.5rem;
        \\  border: 1px solid var(--color-border-default);
        \\  border-radius: 8px;
        \\  background: var(--color-canvas-default);
        \\  transition: box-shadow 0.2s ease;
        \\}
        \\
        \\.definition-card:hover {
        \\  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);
        \\}
        \\
        \\.definition-card h3 {
        \\  margin-top: 0;
        \\  color: var(--color-accent-fg);
        \\  border-bottom: 2px solid var(--color-accent-emphasis);
        \\  padding-bottom: 0.5rem;
        \\}
        \\
        \\.definition-card strong {
        \\  color: var(--color-fg-default);
        \\}
        \\
        \\.definition-card ul, .definition-card ol {
        \\  margin: 1rem 0;
        \\  padding-left: 1.5rem;
        \\}
        \\
        \\.definition-card li {
        \\  margin: 0.5rem 0;
        \\}
        \\
        \\.definition-card code {
        \\  background: var(--color-canvas-subtle);
        \\  padding: 0.2rem 0.4rem;
        \\  border-radius: 3px;
        \\  font-size: 0.9rem;
        \\}
        \\
        \\@media (max-width: 768px) {
        \\  .definition-categories {
        \\    flex-direction: column;
        \\  }
        \\  
        \\  .category-filter {
        \\    text-align: center;
        \\  }
        \\  
        \\  .definition-card {
        \\    margin: 1rem -1rem;
        \\    border-radius: 0;
        \\    border-left: none;
        \\    border-right: none;
        \\  }
        \\}
        \\</style>
        \\
        \\<script>
        \\document.addEventListener('DOMContentLoaded', function() {
        \\  const searchInput = document.getElementById('definition-search');
        \\  const categoryFilters = document.querySelectorAll('.category-filter');
        \\  const definitionCards = document.querySelectorAll('.definition-card');
        \\  
        \\  let currentCategory = 'all';
        \\  
        \\  // Search functionality
        \\  searchInput.addEventListener('input', function() {
        \\    const query = this.value.toLowerCase();
        \\    filterDefinitions(query, currentCategory);
        \\  });
        \\  
        \\  // Category filtering
        \\  categoryFilters.forEach(button => {
        \\    button.addEventListener('click', function() {
        \\      // Update active state
        \\      categoryFilters.forEach(b => b.classList.remove('active'));
        \\      this.classList.add('active');
        \\      
        \\      currentCategory = this.dataset.category;
        \\      const query = searchInput.value.toLowerCase();
        \\      filterDefinitions(query, currentCategory);
        \\    });
        \\  });
        \\  
        \\  function filterDefinitions(searchQuery, category) {
        \\    definitionCards.forEach(card => {
        \\      const text = card.textContent.toLowerCase();
        \\      const categories = card.dataset.category ? card.dataset.category.split(' ') : [];
        \\      
        \\      const matchesSearch = !searchQuery || text.includes(searchQuery);
        \\      const matchesCategory = category === 'all' || categories.includes(category);
        \\      
        \\      if (matchesSearch && matchesCategory) {
        \\        card.style.display = 'block';
        \\        // Highlight search terms
        \\        if (searchQuery) {
        \\          highlightSearchTerms(card, searchQuery);
        \\        }
        \\      } else {
        \\        card.style.display = 'none';
        \\      }
        \\    });
        \\  }
        \\  
        \\  function highlightSearchTerms(element, query) {
        \\    // Simple highlighting implementation
        \\    // In a real implementation, you'd want more sophisticated text highlighting
        \\    const textNodes = getTextNodes(element);
        \\    textNodes.forEach(node => {
        \\      if (node.textContent.toLowerCase().includes(query)) {
        \\        const parent = node.parentNode;
        \\        const regex = new RegExp(`(${query})`, 'gi');
        \\        const highlighted = node.textContent.replace(regex, '<mark>$1</mark>');
        \\        const wrapper = document.createElement('span');
        \\        wrapper.innerHTML = highlighted;
        \\        parent.replaceChild(wrapper, node);
        \\      }
        \\    });
        \\  }
        \\  
        \\  function getTextNodes(element) {
        \\    const textNodes = [];
        \\    const walker = document.createTreeWalker(
        \\      element,
        \\      NodeFilter.SHOW_TEXT,
        \\      null,
        \\      false
        \\    );
        \\    
        \\    let node;
        \\    while (node = walker.nextNode()) {
        \\      textNodes.push(node);
        \\    }
        \\    return textNodes;
        \\  }
        \\});
        \\</script>
        \\
    ;

    try file.writeAll(content);
}
