# =============================================================================
# Prometheus Alert Rules for ABI/Abbey
# =============================================================================

groups:
  # =========================================================================
  # Abbey Application Alerts
  # =========================================================================
  - name: abbey_application
    rules:
      # Service is down
      - alert: AbbeyDown
        expr: up{job="abbey"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Abbey service is down"
          description: "Abbey has been unreachable for more than 1 minute."

      # High error rate
      - alert: AbbeyHighErrorRate
        expr: |
          (
            sum(rate(abbey_errors_total[5m])) /
            sum(rate(abbey_requests_total[5m]))
          ) > 0.05
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High error rate detected (> 5%)"
          description: "Error rate is {{ $value | humanizePercentage }}"

      # Critical error rate
      - alert: AbbeyCriticalErrorRate
        expr: |
          (
            sum(rate(abbey_errors_total[5m])) /
            sum(rate(abbey_requests_total[5m]))
          ) > 0.25
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Critical error rate detected (> 25%)"
          description: "Error rate is {{ $value | humanizePercentage }}"

      # High latency (p95)
      - alert: AbbeyHighLatencyP95
        expr: |
          histogram_quantile(0.95,
            sum(rate(abbey_request_duration_seconds_bucket[5m])) by (le)
          ) > 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High p95 latency (> 1s)"
          description: "95th percentile latency is {{ $value | humanizeDuration }}"

      # High latency (p99)
      - alert: AbbeyHighLatencyP99
        expr: |
          histogram_quantile(0.99,
            sum(rate(abbey_request_duration_seconds_bucket[5m])) by (le)
          ) > 5
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Critical p99 latency (> 5s)"
          description: "99th percentile latency is {{ $value | humanizeDuration }}"

      # Low request rate (potential issue)
      - alert: AbbeyLowRequestRate
        expr: sum(rate(abbey_requests_total[5m])) < 0.1
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Low request rate"
          description: "Request rate is below 0.1 req/s for 10 minutes"

  # =========================================================================
  # AI/LLM Specific Alerts
  # =========================================================================
  - name: abbey_ai
    rules:
      # AI backend failures
      - alert: AbbeyAIBackendFailure
        expr: |
          sum(rate(abbey_ai_backend_errors_total[5m])) by (backend) > 0.1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "AI backend {{ $labels.backend }} experiencing failures"
          description: "Error rate: {{ $value }}/s"

      # High token usage
      - alert: AbbeyHighTokenUsage
        expr: |
          sum(rate(abbey_ai_tokens_used_total[1h])) > 100000
        for: 30m
        labels:
          severity: warning
        annotations:
          summary: "High AI token usage"
          description: "Token usage rate: {{ $value }}/hour"

      # Long AI response time
      - alert: AbbeySlowAIResponse
        expr: |
          histogram_quantile(0.95,
            sum(rate(abbey_ai_response_duration_seconds_bucket[5m])) by (le, backend)
          ) > 30
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Slow AI responses from {{ $labels.backend }}"
          description: "p95 response time: {{ $value | humanizeDuration }}"

  # =========================================================================
  # Resource Alerts
  # =========================================================================
  - name: abbey_resources
    rules:
      # High memory usage
      - alert: AbbeyHighMemoryUsage
        expr: |
          process_resident_memory_bytes{job="abbey"} /
          (1024 * 1024 * 1024) > 0.8
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage (> 80%)"
          description: "Memory usage: {{ $value | humanize }}GB"

      # High goroutine/thread count
      - alert: AbbeyHighThreadCount
        expr: |
          abbey_active_threads > 1000
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High thread count"
          description: "Active threads: {{ $value }}"

      # Connection pool exhaustion
      - alert: AbbeyConnectionPoolExhausted
        expr: |
          abbey_connection_pool_available < 5
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Connection pool nearly exhausted"
          description: "Only {{ $value }} connections available"

  # =========================================================================
  # Infrastructure Alerts
  # =========================================================================
  - name: abbey_infrastructure
    rules:
      # Prometheus target down
      - alert: PrometheusTargetMissing
        expr: up == 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Prometheus target {{ $labels.job }} is down"
          description: "Target has been unreachable for 5 minutes"

      # Prometheus config reload failed
      - alert: PrometheusConfigReloadFailed
        expr: prometheus_config_last_reload_successful == 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Prometheus configuration reload failed"
          description: "Check Prometheus logs for details"

      # Grafana down
      - alert: GrafanaDown
        expr: up{job="grafana"} == 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Grafana is down"
          description: "Grafana dashboard is unreachable"
